{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Proof of Concept Valeur R\u00e9siduelle - Documentation","text":"<p>\u00c0 propos du projet</p> <p>Projet de d\u00e9veloppement d'un mod\u00e8le de machine learning pour l'estimation des prix de v\u00e9hicules d'occasion et de leur courbe de d\u00e9cote, avec une d\u00e9mo de l'application web int\u00e9gr\u00e9e.</p>"},{"location":"#vue-densemble","title":"Vue d'ensemble","text":"<p>Ce projet se d\u00e9roule en deux phases principales :</p>"},{"location":"#phase-1-developpement-du-modele-ml","title":"Phase 1 : D\u00e9veloppement du mod\u00e8le ML","text":"<ul> <li>Objectif : Cr\u00e9er un mod\u00e8le de machine learning pour estimer le prix de vente des v\u00e9hicules d'occasion</li> <li>Donn\u00e9es : Scraping d'annonces des voitures d'occasion sur autohero.com pour la Valeur R\u00e9siduelle et scraping fiches techniques de Caradisiac pour obtenir le Prix neuf</li> <li>Crit\u00e8res : V\u00e9hicules avec mise en circulation &gt;= 2017, kilom\u00e9trage &lt;= 100 000 km</li> <li>M\u00e9trique cl\u00e9 : Taux de d\u00e9cote = VR/Prix neuf</li> </ul>"},{"location":"#phase-2-application-web","title":"Phase 2 : Application web","text":"<ul> <li>Objectif : D\u00e9velopper une application web avec Dash</li> <li>Fonctionnalit\u00e9 : Calculateur de valeur r\u00e9siduelle int\u00e9grant le mod\u00e8le ML</li> <li>Interface : Interface utilisateur pour les estimations</li> </ul>"},{"location":"#structure-du-projet","title":"Structure du projet","text":"<pre><code>Valeur-Residuelle/\n\u251c\u2500\u2500 data/                    # Donn\u00e9es brutes et trait\u00e9es\n\u251c\u2500\u2500 notebooks/               # Notebooks Jupyter d'analyse\n\u251c\u2500\u2500 src/                     # Code source principal\n\u251c\u2500\u2500 models/                  # Mod\u00e8les entra\u00een\u00e9s et pipelines\n\u251c\u2500\u2500 app.py                   # Application web principale\n\u2514\u2500\u2500 docs/                    # Documentation\n</code></pre>"},{"location":"#technologies-utilisees","title":"Technologies utilis\u00e9es","text":"<ul> <li>Machine Learning : scikit-learn, XGBoost, CatBoost</li> <li>Traitement des donn\u00e9es : pandas, numpy</li> <li>Visualisation : matplotlib, seaborn, plotly</li> <li>Application web : Dash</li> <li>Scraping : Selenium</li> </ul>"},{"location":"#demarrage-rapide","title":"D\u00e9marrage rapide","text":"<p>Pour commencer avec le projet :</p> <ol> <li>Installation : Voir Installation</li> <li>Exploration des donn\u00e9es : Consultez EDA</li> <li>Mod\u00e8les : D\u00e9couvrez la Mod\u00e9lisation</li> <li>Application : Testez l'Interface</li> </ol>"},{"location":"#navigation","title":"Navigation","text":""},{"location":"#donnees","title":"\ud83d\udcca Donn\u00e9es","text":"<ul> <li>Sources de donn\u00e9es - Origine et collecte des donn\u00e9es</li> <li>Preprocessing - Nettoyage et transformation</li> <li>Exploration - Analyse exploratoire</li> </ul>"},{"location":"#modeles","title":"\ud83e\udd16 Mod\u00e8les","text":"<ul> <li>Mod\u00e9lisation - D\u00e9veloppement des mod\u00e8les</li> <li>\u00c9valuation - M\u00e9triques et validation</li> <li>Performances - R\u00e9sultats et comparaisons</li> </ul>"},{"location":"#application","title":"\ud83c\udf10 Application","text":"<ul> <li>Interface - Interface utilisateur</li> <li>API - Documentation de l'API</li> <li>D\u00e9ploiement - Guide de d\u00e9ploiement</li> </ul> <p>Documentation g\u00e9n\u00e9r\u00e9e avec MkDocs Material pour le projet Valeur R\u00e9siduelle - NEXIALOG</p>"},{"location":"app/interface/","title":"Interface utilisateur","text":""},{"location":"app/interface/#vue-densemble","title":"Vue d'ensemble","text":"<p>L'application web Valeur R\u00e9siduelle propose une interface d\u00e9velopp\u00e9e avec Dash pour permettre aux utilisateurs d'estimer la valeur de leur v\u00e9hicule d'occasion.</p>"},{"location":"app/interface/#architecture-de-linterface","title":"Architecture de l'interface","text":"<pre><code>graph TD\n    A[Interface Web] --&gt; B[Formulaire de saisie]\n    A --&gt; C[R\u00e9sultats de pr\u00e9diction]\n    A --&gt; D[Graphiques d'analyse]\n\n    B --&gt; B1[Informations v\u00e9hicule]\n    B --&gt; B2[Caract\u00e9ristiques techniques]\n    B --&gt; B3[\u00c9tat et usage]\n\n    C --&gt; C1[Prix estim\u00e9]\n    C --&gt; C2[Taux de d\u00e9cote]\n    C --&gt; C3[Marge de confiance]\n\n    D --&gt; D1[Courbe de d\u00e9cote]\n    D --&gt; D2[Comparaison march\u00e9]\n    D --&gt; D3[Facteurs d'influence]</code></pre>"},{"location":"app/interface/#composants-principaux","title":"Composants principaux","text":""},{"location":"app/interface/#1-formulaire-de-saisie","title":"1. Formulaire de saisie","text":"<pre><code>import dash\nfrom dash import dcc, html, Input, Output, State\nimport dash_bootstrap_components as dbc\n\n# Layout du formulaire principal\ndef create_input_form():\n    return dbc.Container([\n        dbc.Row([\n            dbc.Col([\n                html.H3(\"Informations du v\u00e9hicule\", className=\"mb-3\"),\n\n                # Marque\n                dbc.Label(\"Marque\"),\n                dcc.Dropdown(\n                    id='marque-dropdown',\n                    options=[\n                        {'label': 'BMW', 'value': 'BMW'},\n                        {'label': 'Mercedes', 'value': 'Mercedes'},\n                        {'label': 'Audi', 'value': 'Audi'},\n                        {'label': 'Volkswagen', 'value': 'Volkswagen'},\n                        {'label': 'Renault', 'value': 'Renault'},\n                        {'label': 'Peugeot', 'value': 'Peugeot'},\n                        # ... autres marques\n                    ],\n                    placeholder=\"S\u00e9lectionnez une marque\",\n                    className=\"mb-3\"\n                ),\n\n                # Mod\u00e8le (d\u00e9pendant de la marque)\n                dbc.Label(\"Mod\u00e8le\"),\n                dcc.Dropdown(\n                    id='modele-dropdown',\n                    placeholder=\"S\u00e9lectionnez d'abord une marque\",\n                    className=\"mb-3\"\n                ),\n\n            ], width=6),\n\n            dbc.Col([\n                html.H3(\"Caract\u00e9ristiques\", className=\"mb-3\"),\n\n                # Ann\u00e9e\n                dbc.Label(\"Ann\u00e9e de mise en circulation\"),\n                dcc.Dropdown(\n                    id='annee-dropdown',\n                    options=[{'label': str(year), 'value': year} \n                            for year in range(2024, 2016, -1)],\n                    placeholder=\"Ann\u00e9e\",\n                    className=\"mb-3\"\n                ),\n\n                # Kilom\u00e9trage\n                dbc.Label(\"Kilom\u00e9trage (km)\"),\n                dbc.Input(\n                    id='kilometrage-input',\n                    type='number',\n                    min=0,\n                    max=300000,\n                    step=1000,\n                    placeholder=\"Ex: 45000\",\n                    className=\"mb-3\"\n                ),\n\n            ], width=6)\n        ]),\n\n        dbc.Row([\n            dbc.Col([\n                # Carburant\n                dbc.Label(\"Type de carburant\"),\n                dcc.RadioItems(\n                    id='carburant-radio',\n                    options=[\n                        {'label': ' Essence', 'value': 'Essence'},\n                        {'label': ' Diesel', 'value': 'Diesel'},\n                        {'label': ' Hybride', 'value': 'Hybride'},\n                        {'label': ' \u00c9lectrique', 'value': 'Electrique'}\n                    ],\n                    className=\"mb-3\"\n                ),\n\n                # Transmission\n                dbc.Label(\"Transmission\"),\n                dcc.RadioItems(\n                    id='transmission-radio',\n                    options=[\n                        {'label': ' Manuelle', 'value': 'Manuelle'},\n                        {'label': ' Automatique', 'value': 'Automatique'}\n                    ],\n                    className=\"mb-3\"\n                ),\n\n            ], width=6),\n\n            dbc.Col([\n                # Puissance\n                dbc.Label(\"Puissance (CV)\"),\n                dbc.Input(\n                    id='puissance-input',\n                    type='number',\n                    min=50,\n                    max=600,\n                    step=5,\n                    placeholder=\"Ex: 150\",\n                    className=\"mb-3\"\n                ),\n\n                # Bouton de calcul\n                dbc.Button(\n                    \"Estimer la valeur\",\n                    id=\"estimate-button\",\n                    color=\"primary\",\n                    size=\"lg\",\n                    className=\"mt-4 w-100\"\n                ),\n\n            ], width=6)\n        ])\n    ])\n</code></pre>"},{"location":"app/interface/#2-affichage-des-resultats","title":"2. Affichage des r\u00e9sultats","text":"<pre><code>def create_results_section():\n    return dbc.Container([\n        html.Div(id=\"results-container\", children=[\n            dbc.Alert(\n                \"Veuillez remplir le formulaire et cliquer sur 'Estimer la valeur'\",\n                color=\"info\",\n                className=\"text-center\"\n            )\n        ])\n    ])\n\ndef create_result_cards(estimation_data):\n    \"\"\"Cr\u00e9ation des cartes de r\u00e9sultats\"\"\"\n\n    return dbc.Row([\n        # Prix estim\u00e9\n        dbc.Col([\n            dbc.Card([\n                dbc.CardBody([\n                    html.H4(\"Prix estim\u00e9\", className=\"card-title\"),\n                    html.H2(f\"{estimation_data['prix_estime']:,.0f} \u20ac\", \n                            className=\"text-primary\"),\n                    html.P(f\"Marge: \u00b1{estimation_data['marge']:.0f} \u20ac\",\n                           className=\"text-muted\")\n                ])\n            ], className=\"h-100\")\n        ], width=4),\n\n        # Taux de d\u00e9cote\n        dbc.Col([\n            dbc.Card([\n                dbc.CardBody([\n                    html.H4(\"Taux de d\u00e9cote\", className=\"card-title\"),\n                    html.H2(f\"{estimation_data['taux_decote']:.1%}\", \n                            className=\"text-warning\"),\n                    html.P(f\"Prix neuf: {estimation_data['prix_neuf']:,.0f} \u20ac\",\n                           className=\"text-muted\")\n                ])\n            ], className=\"h-100\")\n        ], width=4),\n\n        # Confiance\n        dbc.Col([\n            dbc.Card([\n                dbc.CardBody([\n                    html.H4(\"Confiance\", className=\"card-title\"),\n                    html.H2(f\"{estimation_data['confiance']:.0%}\", \n                            className=\"text-success\"),\n                    html.P(\"Bas\u00e9 sur 15,000+ v\u00e9hicules\",\n                           className=\"text-muted\")\n                ])\n            ], className=\"h-100\")\n        ], width=4)\n    ], className=\"mb-4\")\n</code></pre>"},{"location":"app/interface/#visualisations-interactives","title":"Visualisations interactives","text":""},{"location":"app/interface/#1-courbe-de-decote","title":"1. Courbe de d\u00e9cote","text":"<pre><code>import plotly.graph_objects as go\nimport plotly.express as px\n\ndef create_depreciation_curve(car_data):\n    \"\"\"Graphique de la courbe de d\u00e9cote dans le temps\"\"\"\n\n    # Simulation de l'\u00e9volution de la valeur\n    years = list(range(car_data['annee'], 2025))\n    values = []\n\n    for year in years:\n        age = year - car_data['annee']\n        # Mod\u00e8le de d\u00e9cote empirique\n        depreciation_rate = 0.15 * age + 0.05 * age**1.2\n        current_value = car_data['prix_neuf'] * (1 - depreciation_rate)\n        values.append(max(current_value, car_data['prix_neuf'] * 0.1))\n\n    fig = go.Figure()\n\n    # Courbe de d\u00e9cote\n    fig.add_trace(go.Scatter(\n        x=years,\n        y=values,\n        mode='lines+markers',\n        name='Valeur estim\u00e9e',\n        line=dict(color='blue', width=3),\n        marker=dict(size=8)\n    ))\n\n    # Point actuel\n    current_year = 2024\n    current_value = car_data['prix_estime']\n    fig.add_trace(go.Scatter(\n        x=[current_year],\n        y=[current_value],\n        mode='markers',\n        name='Valeur actuelle',\n        marker=dict(color='red', size=15, symbol='diamond')\n    ))\n\n    fig.update_layout(\n        title=\"\u00c9volution de la valeur dans le temps\",\n        xaxis_title=\"Ann\u00e9e\",\n        yaxis_title=\"Valeur (\u20ac)\",\n        hovermode='x unified',\n        template='plotly_white'\n    )\n\n    return fig\n</code></pre>"},{"location":"app/interface/#2-comparaison-avec-le-marche","title":"2. Comparaison avec le march\u00e9","text":"<pre><code>def create_market_comparison(car_data, market_data):\n    \"\"\"Graphique de comparaison avec le march\u00e9\"\"\"\n\n    fig = go.Figure()\n\n    # Distribution des prix du march\u00e9 pour des v\u00e9hicules similaires\n    fig.add_trace(go.Histogram(\n        x=market_data['prix'],\n        name='March\u00e9',\n        opacity=0.7,\n        nbinsx=20,\n        marker_color='lightblue'\n    ))\n\n    # Votre estimation\n    fig.add_vline(\n        x=car_data['prix_estime'],\n        line_dash=\"dash\",\n        line_color=\"red\",\n        annotation_text=f\"Votre v\u00e9hicule: {car_data['prix_estime']:,.0f}\u20ac\"\n    )\n\n    fig.update_layout(\n        title=\"Position par rapport au march\u00e9\",\n        xaxis_title=\"Prix (\u20ac)\",\n        yaxis_title=\"Nombre de v\u00e9hicules\",\n        template='plotly_white'\n    )\n\n    return fig\n</code></pre>"},{"location":"app/interface/#fonctionnalites-avancees","title":"Fonctionnalit\u00e9s avanc\u00e9es","text":""},{"location":"app/interface/#1-mode-comparaison","title":"1. Mode comparaison","text":"<pre><code>@app.callback(\n    Output('comparison-results', 'children'),\n    [Input('compare-button', 'n_clicks')],\n    [State('car1-data', 'data'),\n     State('car2-data', 'data')]\n)\ndef compare_vehicles(n_clicks, car1_data, car2_data):\n    if not n_clicks or not car1_data or not car2_data:\n        return \"Ajoutez deux v\u00e9hicules pour les comparer\"\n\n    # Comparaison c\u00f4te \u00e0 c\u00f4te\n    comparison_table = dbc.Table([\n        html.Thead([\n            html.Tr([\n                html.Th(\"Caract\u00e9ristique\"),\n                html.Th(\"V\u00e9hicule 1\"),\n                html.Th(\"V\u00e9hicule 2\"),\n                html.Th(\"Diff\u00e9rence\")\n            ])\n        ]),\n        html.Tbody([\n            html.Tr([\n                html.Td(\"Prix estim\u00e9\"),\n                html.Td(f\"{car1_data['prix_estime']:,.0f} \u20ac\"),\n                html.Td(f\"{car2_data['prix_estime']:,.0f} \u20ac\"),\n                html.Td(f\"{car2_data['prix_estime'] - car1_data['prix_estime']:+,.0f} \u20ac\")\n            ]),\n            html.Tr([\n                html.Td(\"Taux de d\u00e9cote\"),\n                html.Td(f\"{car1_data['taux_decote']:.1%}\"),\n                html.Td(f\"{car2_data['taux_decote']:.1%}\"),\n                html.Td(f\"{car2_data['taux_decote'] - car1_data['taux_decote']:+.1%}\")\n            ])\n        ])\n    ], striped=True, bordered=True, hover=True)\n\n    return comparison_table\n</code></pre>"},{"location":"app/interface/#2-historique-des-recherches","title":"2. Historique des recherches","text":"<pre><code>def save_search_history(user_session, car_data, estimation):\n    \"\"\"Sauvegarde de l'historique des recherches\"\"\"\n\n    search_entry = {\n        'timestamp': datetime.now().isoformat(),\n        'car_data': car_data,\n        'estimation': estimation,\n        'user_session': user_session\n    }\n\n    # Sauvegarde en base de donn\u00e9es\n    # (impl\u00e9mentation selon votre syst\u00e8me de persistence)\n\n    return search_entry\n\ndef display_search_history(user_session):\n    \"\"\"Affichage de l'historique\"\"\"\n\n    # R\u00e9cup\u00e9ration des recherches pr\u00e9c\u00e9dentes\n    history = get_user_search_history(user_session)\n\n    if not history:\n        return dbc.Alert(\"Aucune recherche pr\u00e9c\u00e9dente\", color=\"info\")\n\n    history_cards = []\n    for search in history[-5:]:  # 5 derni\u00e8res recherches\n        card = dbc.Card([\n            dbc.CardBody([\n                html.H6(f\"{search['car_data']['marque']} {search['car_data']['modele']}\"),\n                html.P(f\"Prix estim\u00e9: {search['estimation']['prix_estime']:,.0f} \u20ac\"),\n                html.Small(f\"Le {search['timestamp'][:10]}\", className=\"text-muted\")\n            ])\n        ], className=\"mb-2\")\n        history_cards.append(card)\n\n    return html.Div(history_cards)\n</code></pre>"},{"location":"app/interface/#responsive-design","title":"Responsive design","text":""},{"location":"app/interface/#configuration-bootstrap","title":"Configuration Bootstrap","text":"<pre><code># Th\u00e8me Bootstrap personnalis\u00e9\napp = dash.Dash(__name__, \n                external_stylesheets=[dbc.themes.BOOTSTRAP],\n                meta_tags=[\n                    {\"name\": \"viewport\", \n                     \"content\": \"width=device-width, initial-scale=1\"}\n                ])\n\n# CSS personnalis\u00e9\napp.index_string = '''\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        {%metas%}\n        &lt;title&gt;Valeur R\u00e9siduelle - Estimation v\u00e9hicules&lt;/title&gt;\n        {%favicon%}\n        {%css%}\n        &lt;style&gt;\n            .card {\n                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n                border: none;\n                border-radius: 10px;\n            }\n            .btn-primary {\n                background: linear-gradient(45deg, #007bff, #0056b3);\n                border: none;\n            }\n            .navbar-brand {\n                font-weight: bold;\n                color: #007bff !important;\n            }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        {%app_entry%}\n        {%config%}\n        {%scripts%}\n        {%renderer%}\n    &lt;/body&gt;\n&lt;/html&gt;\n'''\n</code></pre>"},{"location":"app/interface/#gestion-des-erreurs-et-validation","title":"Gestion des erreurs et validation","text":""},{"location":"app/interface/#validation-des-entrees","title":"Validation des entr\u00e9es","text":"<pre><code>@app.callback(\n    [Output('estimate-button', 'disabled'),\n     Output('validation-feedback', 'children')],\n    [Input('marque-dropdown', 'value'),\n     Input('modele-dropdown', 'value'),\n     Input('annee-dropdown', 'value'),\n     Input('kilometrage-input', 'value'),\n     Input('puissance-input', 'value')]\n)\ndef validate_inputs(marque, modele, annee, kilometrage, puissance):\n    \"\"\"Validation en temps r\u00e9el des entr\u00e9es utilisateur\"\"\"\n\n    errors = []\n\n    if not marque:\n        errors.append(\"Veuillez s\u00e9lectionner une marque\")\n    if not modele:\n        errors.append(\"Veuillez s\u00e9lectionner un mod\u00e8le\")\n    if not annee:\n        errors.append(\"Veuillez s\u00e9lectionner une ann\u00e9e\")\n    if not kilometrage or kilometrage &lt; 0:\n        errors.append(\"Veuillez entrer un kilom\u00e9trage valide\")\n    if not puissance or puissance &lt; 50:\n        errors.append(\"Veuillez entrer une puissance valide\")\n\n    # Validations sp\u00e9cifiques\n    if annee and kilometrage:\n        age = 2024 - annee\n        max_km_realistic = age * 25000  # 25k km/an max\n        if kilometrage &gt; max_km_realistic:\n            errors.append(f\"Kilom\u00e9trage trop \u00e9lev\u00e9 pour un v\u00e9hicule de {age} ans\")\n\n    if errors:\n        feedback = dbc.Alert([\n            html.Ul([html.Li(error) for error in errors])\n        ], color=\"danger\")\n        return True, feedback\n    else:\n        return False, dbc.Alert(\"Tous les champs sont valides \u2713\", color=\"success\")\n</code></pre>"},{"location":"app/interface/#accessibilite","title":"Accessibilit\u00e9","text":""},{"location":"app/interface/#standards-wcag","title":"Standards WCAG","text":"<pre><code># Am\u00e9lioration de l'accessibilit\u00e9\ndef create_accessible_form():\n    return html.Div([\n        # Navigation clavier\n        html.Div(\n            \"Passer au contenu principal\",\n            id=\"skip-link\",\n            style={\"position\": \"absolute\", \"left\": \"-9999px\"},\n            tabIndex=1\n        ),\n\n        # Labels explicites\n        dbc.Label(\"Marque du v\u00e9hicule\", html_for=\"marque-dropdown\"),\n        dcc.Dropdown(\n            id='marque-dropdown',\n            # ARIA labels\n            **{\"aria-label\": \"S\u00e9lection de la marque du v\u00e9hicule\"}\n        ),\n\n        # Messages d'erreur avec ARIA\n        html.Div(\n            id=\"error-messages\",\n            **{\"aria-live\": \"polite\", \"aria-atomic\": \"true\"}\n        )\n    ])\n</code></pre>"},{"location":"data/eda/","title":"Analyse exploratoire des donn\u00e9es (EDA)","text":""},{"location":"data/eda/#vue-densemble","title":"Vue d'ensemble","text":"<p>L'analyse exploratoire des donn\u00e9es (EDA) est essentielle pour comprendre les patterns, distributions et relations dans notre dataset de v\u00e9hicules d'occasion. Cette analyse guide les d\u00e9cisions de preprocessing et de mod\u00e9lisation.</p>"},{"location":"data/eda/#structure-du-dataset","title":"Structure du dataset","text":""},{"location":"data/eda/#statistiques-generales","title":"Statistiques g\u00e9n\u00e9rales","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Chargement des donn\u00e9es\ndf = pd.read_csv('data/processed_data/preprocessed_data.csv')\n\nprint(f\"Dataset: {len(df):,} v\u00e9hicules, {len(df.columns)} variables\")\nprint(f\"P\u00e9riode: {df['annee'].min()} - {df['annee'].max()}\")\nprint(f\"Valeur r\u00e9siduelle moyenne: {df['taux_decote'].mean():.1%}\")\n</code></pre> M\u00e9trique Valeur Nombre de v\u00e9hicules 2352 Variables 34 Ann\u00e9es couvertes 2017-2024 Marques 34 Mod\u00e8les 221"},{"location":"data/eda/#analyse-univariee","title":"Analyse univari\u00e9e","text":""},{"location":"data/eda/#distribution-de-la-variable-cible","title":"Distribution de la variable cible","text":"<pre><code>def analyze_target_variable(df):\n    \"\"\"Analyse de la distribution du taux de d\u00e9cote\"\"\"\n\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=['Distribution', 'Box Plot', 'Q-Q Plot', '\u00c9volution temporelle'],\n        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n    )\n\n    # 1. Histogramme\n    fig.add_trace(\n        go.Histogram(x=df['taux_decote'], nbinsx=50, name='Distribution'),\n        row=1, col=1\n    )\n\n    # 2. Box plot\n    fig.add_trace(\n        go.Box(y=df['taux_decote'], name='Taux de d\u00e9cote'),\n        row=1, col=2\n    )\n\n    # 3. Q-Q plot (approximation)\n    from scipy import stats\n    theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, 100))\n    sample_quantiles = np.percentile(df['taux_decote'], np.linspace(1, 99, 100))\n\n    fig.add_trace(\n        go.Scatter(x=theoretical_quantiles, y=sample_quantiles, \n                  mode='markers', name='Q-Q Plot'),\n        row=2, col=1\n    )\n\n    # 4. \u00c9volution par ann\u00e9e\n    yearly_stats = df.groupby('annee')['taux_decote'].agg(['mean', 'std']).reset_index()\n\n    fig.add_trace(\n        go.Scatter(x=yearly_stats['annee'], y=yearly_stats['mean'],\n                  mode='lines+markers', name='Moyenne annuelle',\n                  error_y=dict(type='data', array=yearly_stats['std'])),\n        row=2, col=2\n    )\n\n    fig.update_layout(height=800, title_text=\"Analyse de la variable cible: Taux de d\u00e9cote\")\n    fig.show()\n\n    # Statistiques descriptives\n    print(\"Statistiques du taux de d\u00e9cote:\")\n    print(df['taux_decote'].describe())\n\n# Ex\u00e9cution de l'analyse\nanalyze_target_variable(df)\n</code></pre> <p>Observations cl\u00e9s : - Distribution l\u00e9g\u00e8rement asym\u00e9trique (skewness = -0.23) - Taux de d\u00e9cote m\u00e9dian : 67.3% - \u00c9cart-type : 14.2% - Valeurs aberrantes : &lt; 1% du dataset</p>"},{"location":"data/eda/#variables-numeriques-principales","title":"Variables num\u00e9riques principales","text":"<pre><code>def analyze_numeric_variables(df):\n    \"\"\"Analyse des variables num\u00e9riques importantes\"\"\"\n\n    numeric_vars = ['kilometrage', 'puissance', 'age_vehicule', 'km_par_an', 'prix_neuf']\n\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    axes = axes.ravel()\n\n    for i, var in enumerate(numeric_vars):\n        # Histogramme avec densit\u00e9\n        axes[i].hist(df[var], bins=50, alpha=0.7, edgecolor='black', density=True)\n\n        # Ajout de la courbe de densit\u00e9\n        from scipy.stats import gaussian_kde\n        kde = gaussian_kde(df[var].dropna())\n        x_range = np.linspace(df[var].min(), df[var].max(), 100)\n        axes[i].plot(x_range, kde(x_range), 'r-', linewidth=2)\n\n        axes[i].set_title(f'Distribution: {var}')\n        axes[i].set_xlabel(var)\n        axes[i].set_ylabel('Densit\u00e9')\n\n        # Statistiques sur le graphique\n        mean_val = df[var].mean()\n        median_val = df[var].median()\n        axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Moyenne: {mean_val:.0f}')\n        axes[i].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'M\u00e9diane: {median_val:.0f}')\n        axes[i].legend()\n\n    # Suppression du dernier subplot vide\n    fig.delaxes(axes[-1])\n\n    plt.tight_layout()\n    plt.show()\n\n    # Matrice de corr\u00e9lation\n    correlation_matrix = df[numeric_vars + ['taux_decote']].corr()\n\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,\n                square=True, fmt='.3f')\n    plt.title('Matrice de corr\u00e9lation - Variables num\u00e9riques')\n    plt.tight_layout()\n    plt.show()\n\n    return correlation_matrix\n\n# Analyse des corr\u00e9lations\ncorr_matrix = analyze_numeric_variables(df)\n</code></pre>"},{"location":"data/eda/#variables-categorielles","title":"Variables cat\u00e9gorielles","text":"<pre><code>def analyze_categorical_variables(df):\n    \"\"\"Analyse des variables cat\u00e9gorielles\"\"\"\n\n    categorical_vars = ['marque', 'carburant', 'transmission', 'segment']\n\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    axes = axes.ravel()\n\n    for i, var in enumerate(categorical_vars):\n        # Comptage des modalit\u00e9s\n        value_counts = df[var].value_counts()\n\n        # Limitation aux top 10 pour la lisibilit\u00e9\n        if len(value_counts) &gt; 10:\n            top_values = value_counts.head(10)\n            others_count = value_counts.iloc[10:].sum()\n            if others_count &gt; 0:\n                top_values['Autres'] = others_count\n            value_counts = top_values\n\n        # Graphique en barres horizontales\n        y_pos = np.arange(len(value_counts))\n        axes[i].barh(y_pos, value_counts.values)\n        axes[i].set_yticks(y_pos)\n        axes[i].set_yticklabels(value_counts.index)\n        axes[i].set_xlabel('Nombre de v\u00e9hicules')\n        axes[i].set_title(f'Distribution: {var}')\n\n        # Ajout des pourcentages\n        for j, v in enumerate(value_counts.values):\n            percentage = (v / len(df)) * 100\n            axes[i].text(v + max(value_counts.values) * 0.01, j, \n                        f'{percentage:.1f}%', va='center')\n\n    plt.tight_layout()\n    plt.show()\n\nanalyze_categorical_variables(df)\n</code></pre>"},{"location":"data/eda/#analyse-bivariee","title":"Analyse bivari\u00e9e","text":""},{"location":"data/eda/#relations-avec-la-variable-cible","title":"Relations avec la variable cible","text":"<pre><code>def analyze_target_relationships(df):\n    \"\"\"Analyse des relations entre variables et taux de d\u00e9cote\"\"\"\n\n    # 1. Taux de d\u00e9cote par marque\n    plt.figure(figsize=(15, 8))\n\n    # Calcul des statistiques par marque\n    brand_stats = df.groupby('marque')['taux_decote'].agg(['mean', 'std', 'count']).reset_index()\n    brand_stats = brand_stats[brand_stats['count'] &gt;= 50]  # Minimum 50 v\u00e9hicules\n    brand_stats = brand_stats.sort_values('mean')\n\n    # Box plot par marque\n    brands_filtered = brand_stats['marque'].tolist()\n    df_filtered = df[df['marque'].isin(brands_filtered)]\n\n    sns.boxplot(data=df_filtered, x='marque', y='taux_decote')\n    plt.xticks(rotation=45)\n    plt.title('Distribution du taux de d\u00e9cote par marque')\n    plt.ylabel('Taux de d\u00e9cote')\n    plt.tight_layout()\n    plt.show()\n\n    # 2. \u00c9volution avec l'\u00e2ge\n    plt.figure(figsize=(12, 6))\n\n    # R\u00e9gression polynomiale pour visualiser la tendance\n    from sklearn.preprocessing import PolynomialFeatures\n    from sklearn.linear_model import LinearRegression\n\n    X = df['age_vehicule'].values.reshape(-1, 1)\n    y = df['taux_decote'].values\n\n    poly_features = PolynomialFeatures(degree=2)\n    X_poly = poly_features.fit_transform(X)\n\n    model = LinearRegression()\n    model.fit(X_poly, y)\n\n    # Pr\u00e9diction pour la courbe lisse\n    X_smooth = np.linspace(df['age_vehicule'].min(), df['age_vehicule'].max(), 100).reshape(-1, 1)\n    X_smooth_poly = poly_features.transform(X_smooth)\n    y_smooth = model.predict(X_smooth_poly)\n\n    # Scatter plot avec tendance\n    plt.scatter(df['age_vehicule'], df['taux_decote'], alpha=0.5, s=20)\n    plt.plot(X_smooth, y_smooth, 'r-', linewidth=3, label='Tendance polynomiale')\n\n    plt.xlabel('\u00c2ge du v\u00e9hicule (ann\u00e9es)')\n    plt.ylabel('Taux de d\u00e9cote')\n    plt.title('Relation entre \u00e2ge et taux de d\u00e9cote')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    # 3. Impact du kilom\u00e9trage\n    plt.figure(figsize=(12, 6))\n\n    # Binning du kilom\u00e9trage pour une meilleure visualisation\n    df['km_binned'] = pd.cut(df['kilometrage'], \n                            bins=[0, 20000, 50000, 80000, 120000, float('inf')],\n                            labels=['0-20k', '20-50k', '50-80k', '80-120k', '120k+'])\n\n    sns.violinplot(data=df, x='km_binned', y='taux_decote')\n    plt.title('Impact du kilom\u00e9trage sur le taux de d\u00e9cote')\n    plt.xlabel('Kilom\u00e9trage (km)')\n    plt.ylabel('Taux de d\u00e9cote')\n    plt.show()\n\nanalyze_target_relationships(df)\n</code></pre>"},{"location":"data/eda/#heatmap-des-correlations-avancees","title":"Heatmap des corr\u00e9lations avanc\u00e9es","text":"<pre><code>def advanced_correlation_analysis(df):\n    \"\"\"Analyse avanc\u00e9e des corr\u00e9lations\"\"\"\n\n    # S\u00e9lection des variables num\u00e9riques importantes\n    numeric_cols = [\n        'taux_decote', 'age_vehicule', 'kilometrage', 'km_par_an',\n        'puissance', 'puissance_specifique', 'prix_neuf', 'popularite_marque'\n    ]\n\n    # Matrice de corr\u00e9lation avec diff\u00e9rentes m\u00e9thodes\n    correlations = pd.DataFrame()\n\n    for method in ['pearson', 'spearman', 'kendall']:\n        corr = df[numeric_cols].corr(method=method)['taux_decote'].drop('taux_decote')\n        correlations[method] = corr\n\n    # Visualisation\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n    for i, method in enumerate(['pearson', 'spearman', 'kendall']):\n        corr_values = correlations[method].sort_values(key=abs, ascending=False)\n\n        colors = ['red' if x &lt; 0 else 'green' for x in corr_values.values]\n        axes[i].barh(range(len(corr_values)), corr_values.values, color=colors, alpha=0.7)\n        axes[i].set_yticks(range(len(corr_values)))\n        axes[i].set_yticklabels(corr_values.index)\n        axes[i].set_xlabel(f'Corr\u00e9lation {method.title()}')\n        axes[i].set_title(f'Corr\u00e9lations avec taux_decote ({method})')\n        axes[i].grid(True, alpha=0.3)\n\n        # Ajout des valeurs\n        for j, v in enumerate(corr_values.values):\n            axes[i].text(v + 0.01 if v &gt; 0 else v - 0.01, j, f'{v:.3f}', \n                        va='center', ha='left' if v &gt; 0 else 'right')\n\n    plt.tight_layout()\n    plt.show()\n\n    return correlations\n\ncorrelations_advanced = advanced_correlation_analysis(df)\n</code></pre>"},{"location":"data/eda/#analyse-multivariee","title":"Analyse multivari\u00e9e","text":""},{"location":"data/eda/#analyse-en-composantes-principales-pca","title":"Analyse en composantes principales (PCA)","text":"<pre><code>def pca_analysis(df):\n    \"\"\"Analyse en composantes principales\"\"\"\n\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import StandardScaler\n\n    # S\u00e9lection des variables num\u00e9riques\n    numeric_features = [\n        'age_vehicule', 'kilometrage', 'puissance', 'cylindree',\n        'km_par_an', 'puissance_specifique', 'prix_neuf'\n    ]\n\n    # Standardisation\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(df[numeric_features])\n\n    # PCA\n    pca = PCA()\n    X_pca = pca.fit_transform(X_scaled)\n\n    # Visualisation de la variance expliqu\u00e9e\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n    # 1. Variance expliqu\u00e9e par composante\n    axes[0].bar(range(1, len(pca.explained_variance_ratio_) + 1), \n                pca.explained_variance_ratio_)\n    axes[0].set_xlabel('Composante principale')\n    axes[0].set_ylabel('Variance expliqu\u00e9e')\n    axes[0].set_title('Variance expliqu\u00e9e par composante')\n\n    # 2. Variance cumulative\n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n    axes[1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n    axes[1].axhline(y=0.8, color='r', linestyle='--', label='80% de variance')\n    axes[1].axhline(y=0.95, color='g', linestyle='--', label='95% de variance')\n    axes[1].set_xlabel('Nombre de composantes')\n    axes[1].set_ylabel('Variance cumulative expliqu\u00e9e')\n    axes[1].set_title('Variance cumulative expliqu\u00e9e')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Biplot des deux premi\u00e8res composantes\n    plt.figure(figsize=(12, 8))\n\n    # Points color\u00e9s par taux de d\u00e9cote\n    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n                         c=df['taux_decote'], cmap='viridis', alpha=0.6)\n    plt.colorbar(scatter, label='Taux de d\u00e9cote')\n\n    # Vecteurs des variables originales\n    feature_vectors = pca.components_[:2].T\n\n    for i, feature in enumerate(numeric_features):\n        plt.arrow(0, 0, feature_vectors[i, 0]*3, feature_vectors[i, 1]*3,\n                 head_width=0.1, head_length=0.1, fc='red', ec='red')\n        plt.text(feature_vectors[i, 0]*3.5, feature_vectors[i, 1]*3.5, feature,\n                fontsize=10, ha='center', va='center')\n\n    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} de variance)')\n    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} de variance)')\n    plt.title('Biplot PCA - Projection des v\u00e9hicules et variables')\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n    # Contribution des variables aux composantes principales\n    components_df = pd.DataFrame(\n        pca.components_[:3].T,\n        columns=['PC1', 'PC2', 'PC3'],\n        index=numeric_features\n    )\n\n    print(\"Contribution des variables aux 3 premi\u00e8res composantes:\")\n    print(components_df.round(3))\n\n    return pca, X_pca\n\npca_result, X_pca = pca_analysis(df)\n</code></pre>"},{"location":"data/eda/#clustering-des-vehicules","title":"Clustering des v\u00e9hicules","text":"<pre><code>def clustering_analysis(df):\n    \"\"\"Analyse de clustering pour identifier des groupes de v\u00e9hicules\"\"\"\n\n    from sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import silhouette_score\n\n    # Pr\u00e9paration des donn\u00e9es\n    clustering_features = [\n        'age_vehicule', 'kilometrage', 'puissance', 'prix_neuf', 'taux_decote'\n    ]\n\n    X = df[clustering_features].copy()\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # D\u00e9termination du nombre optimal de clusters\n    inertias = []\n    silhouette_scores = []\n    K_range = range(2, 11)\n\n    for k in K_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X_scaled)\n        inertias.append(kmeans.inertia_)\n        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n\n    # Visualisation des m\u00e9triques\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n    # M\u00e9thode du coude\n    axes[0].plot(K_range, inertias, 'bo-')\n    axes[0].set_xlabel('Nombre de clusters (k)')\n    axes[0].set_ylabel('Inertie')\n    axes[0].set_title('M\u00e9thode du coude')\n    axes[0].grid(True, alpha=0.3)\n\n    # Score de silhouette\n    axes[1].plot(K_range, silhouette_scores, 'ro-')\n    axes[1].set_xlabel('Nombre de clusters (k)')\n    axes[1].set_ylabel('Score de silhouette')\n    axes[1].set_title('Score de silhouette')\n    axes[1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Clustering optimal (k=4 bas\u00e9 sur le coude et silhouette)\n    optimal_k = 4\n    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n    clusters = kmeans_final.fit_predict(X_scaled)\n\n    # Ajout des clusters au dataframe\n    df_clustered = df.copy()\n    df_clustered['cluster'] = clusters\n\n    # Analyse des clusters\n    cluster_summary = df_clustered.groupby('cluster')[clustering_features].mean()\n\n    print(\"Profil des clusters (moyennes):\")\n    print(cluster_summary.round(2))\n\n    # Visualisation 3D des clusters\n    fig = go.Figure()\n\n    colors = ['red', 'blue', 'green', 'orange', 'purple']\n\n    for cluster in range(optimal_k):\n        cluster_data = df_clustered[df_clustered['cluster'] == cluster]\n\n        fig.add_trace(go.Scatter3d(\n            x=cluster_data['age_vehicule'],\n            y=cluster_data['kilometrage'],\n            z=cluster_data['prix_neuf'],\n            mode='markers',\n            marker=dict(size=4, color=colors[cluster], opacity=0.6),\n            name=f'Cluster {cluster}',\n            text=cluster_data['marque'],\n            hovertemplate='&lt;b&gt;%{text}&lt;/b&gt;&lt;br&gt;' +\n                         '\u00c2ge: %{x} ans&lt;br&gt;' +\n                         'Kilom\u00e9trage: %{y:,.0f} km&lt;br&gt;' +\n                         'Prix neuf: %{z:,.0f} \u20ac&lt;br&gt;' +\n                         '&lt;extra&gt;&lt;/extra&gt;'\n        ))\n\n    fig.update_layout(\n        title='Clustering des v\u00e9hicules (3D)',\n        scene=dict(\n            xaxis_title='\u00c2ge (ann\u00e9es)',\n            yaxis_title='Kilom\u00e9trage (km)',\n            zaxis_title='Prix neuf (\u20ac)'\n        ),\n        width=800,\n        height=600\n    )\n\n    fig.show()\n\n    return df_clustered, kmeans_final\n\ndf_clustered, kmeans_model = clustering_analysis(df)\n</code></pre>"},{"location":"data/eda/#insights-et-patterns-decouverts","title":"Insights et patterns d\u00e9couverts","text":""},{"location":"data/eda/#resume-des-decouvertes-principales","title":"R\u00e9sum\u00e9 des d\u00e9couvertes principales","text":"<pre><code>def generate_eda_insights(df, correlations, df_clustered):\n    \"\"\"G\u00e9n\u00e9ration automatique d'insights bas\u00e9s sur l'EDA\"\"\"\n\n    insights = {\n        'patterns_temporels': {},\n        'segments_vehicules': {},\n        'facteurs_cles': {},\n        'recommandations': []\n    }\n\n    # 1. Patterns temporels\n    yearly_depreciation = df.groupby('annee')['taux_decote'].mean()\n    insights['patterns_temporels']['evolution_decote'] = {\n        'tendance': 'stable' if yearly_depreciation.std() &lt; 0.05 else 'variable',\n        'annee_max_decote': yearly_depreciation.idxmax(),\n        'annee_min_decote': yearly_depreciation.idxmin()\n    }\n\n    # 2. Segmentation v\u00e9hicules\n    brand_performance = df.groupby('marque')['taux_decote'].agg(['mean', 'count'])\n    brand_performance = brand_performance[brand_performance['count'] &gt;= 50]\n\n    insights['segments_vehicules']['marques_premium'] = brand_performance[\n        brand_performance['mean'] &gt; brand_performance['mean'].quantile(0.75)\n    ].index.tolist()\n\n    insights['segments_vehicules']['marques_economiques'] = brand_performance[\n        brand_performance['mean'] &lt; brand_performance['mean'].quantile(0.25)\n    ].index.tolist()\n\n    # 3. Facteurs cl\u00e9s\n    correlations_abs = correlations['pearson'].abs().sort_values(ascending=False)\n    insights['facteurs_cles']['top_3_predicteurs'] = correlations_abs.head(3).index.tolist()\n\n    # 4. Analyse des clusters\n    cluster_profiles = df_clustered.groupby('cluster').agg({\n        'taux_decote': 'mean',\n        'age_vehicule': 'mean',\n        'kilometrage': 'mean',\n        'prix_neuf': 'mean'\n    }).round(2)\n\n    insights['clusters'] = cluster_profiles.to_dict('index')\n\n    # 5. Recommandations bas\u00e9es sur les donn\u00e9es\n    if correlations['pearson']['age_vehicule'] &lt; -0.5:\n        insights['recommandations'].append(\n            \"L'\u00e2ge est un pr\u00e9dicteur fort de la d\u00e9cote - consid\u00e9rer des features temporelles avanc\u00e9es\"\n        )\n\n    if correlations['pearson']['kilometrage'] &lt; -0.3:\n        insights['recommandations'].append(\n            \"Le kilom\u00e9trage impact significativement la valeur - int\u00e9grer des ratios km/\u00e2ge\"\n        )\n\n    # Affichage des insights\n    print(\"=== INSIGHTS D\u00c9COUVERTS ===\\n\")\n\n    print(\"1. PATTERNS TEMPORELS:\")\n    print(f\"   - \u00c9volution de la d\u00e9cote: {insights['patterns_temporels']['evolution_decote']['tendance']}\")\n    print(f\"   - Ann\u00e9e avec plus forte d\u00e9cote: {insights['patterns_temporels']['evolution_decote']['annee_max_decote']}\")\n\n    print(\"\\n2. SEGMENTS DE V\u00c9HICULES:\")\n    print(f\"   - Marques premium (forte d\u00e9cote): {', '.join(insights['segments_vehicules']['marques_premium'][:3])}\")\n    print(f\"   - Marques \u00e9conomiques (faible d\u00e9cote): {', '.join(insights['segments_vehicules']['marques_economiques'][:3])}\")\n\n    print(\"\\n3. FACTEURS CL\u00c9S:\")\n    print(f\"   - Top 3 pr\u00e9dicteurs: {', '.join(insights['facteurs_cles']['top_3_predicteurs'])}\")\n\n    print(\"\\n4. PROFILS DE CLUSTERS:\")\n    for cluster_id, profile in insights['clusters'].items():\n        print(f\"   - Cluster {cluster_id}: D\u00e9cote {profile['taux_decote']:.1%}, \"\n              f\"\u00c2ge {profile['age_vehicule']:.1f}ans, \"\n              f\"Prix neuf {profile['prix_neuf']:,.0f}\u20ac\")\n\n    print(\"\\n5. RECOMMANDATIONS:\")\n    for rec in insights['recommandations']:\n        print(f\"   - {rec}\")\n\n    return insights\n\n# G\u00e9n\u00e9ration des insights\ninsights = generate_eda_insights(df, correlations_advanced, df_clustered)\n</code></pre>"},{"location":"data/eda/#dashboard-interactif","title":"Dashboard interactif","text":"<pre><code>def create_interactive_dashboard():\n    \"\"\"Cr\u00e9ation d'un dashboard interactif avec Plotly Dash\"\"\"\n\n    import dash\n    from dash import dcc, html, Input, Output\n    import plotly.express as px\n\n    # Initialisation de l'application Dash\n    app = dash.Dash(__name__)\n\n    app.layout = html.Div([\n        html.H1(\"Dashboard EDA - Valeur R\u00e9siduelle\", \n                style={'textAlign': 'center', 'marginBottom': 30}),\n\n        # Contr\u00f4les\n        html.Div([\n            html.Div([\n                html.Label(\"S\u00e9lectionner la marque:\"),\n                dcc.Dropdown(\n                    id='brand-dropdown',\n                    options=[{'label': 'Toutes', 'value': 'all'}] + \n                           [{'label': brand, 'value': brand} for brand in df['marque'].unique()],\n                    value='all'\n                )\n            ], style={'width': '30%', 'display': 'inline-block'}),\n\n            html.Div([\n                html.Label(\"S\u00e9lectionner l'ann\u00e9e:\"),\n                dcc.RangeSlider(\n                    id='year-slider',\n                    min=df['annee'].min(),\n                    max=df['annee'].max(),\n                    value=[df['annee'].min(), df['annee'].max()],\n                    marks={year: str(year) for year in range(df['annee'].min(), df['annee'].max()+1)},\n                    step=1\n                )\n            ], style={'width': '65%', 'float': 'right', 'display': 'inline-block'})\n        ], style={'marginBottom': 30}),\n\n        # Graphiques\n        html.Div([\n            dcc.Graph(id='depreciation-scatter'),\n            dcc.Graph(id='brand-boxplot'),\n        ], style={'width': '100%'}),\n\n        html.Div([\n            dcc.Graph(id='correlation-heatmap'),\n            dcc.Graph(id='distribution-histogram'),\n        ], style={'width': '100%'})\n    ])\n\n    # Callbacks pour l'interactivit\u00e9\n    @app.callback(\n        [Output('depreciation-scatter', 'figure'),\n         Output('brand-boxplot', 'figure'),\n         Output('correlation-heatmap', 'figure'),\n         Output('distribution-histogram', 'figure')],\n        [Input('brand-dropdown', 'value'),\n         Input('year-slider', 'value')]\n    )\n    def update_graphs(selected_brand, year_range):\n        # Filtrage des donn\u00e9es\n        filtered_df = df[\n            (df['annee'] &gt;= year_range[0]) &amp; \n            (df['annee'] &lt;= year_range[1])\n        ]\n\n        if selected_brand != 'all':\n            filtered_df = filtered_df[filtered_df['marque'] == selected_brand]\n\n        # 1. Scatter plot \u00e2ge vs d\u00e9cote\n        fig1 = px.scatter(\n            filtered_df, \n            x='age_vehicule', \n            y='taux_decote',\n            color='marque' if selected_brand == 'all' else None,\n            title='Relation \u00c2ge - Taux de d\u00e9cote',\n            trendline='ols'\n        )\n\n        # 2. Box plot par marque\n        fig2 = px.box(\n            filtered_df, \n            x='marque', \n            y='taux_decote',\n            title='Distribution du taux de d\u00e9cote par marque'\n        )\n        fig2.update_xaxes(tickangle=45)\n\n        # 3. Heatmap de corr\u00e9lation\n        numeric_cols = ['taux_decote', 'age_vehicule', 'kilometrage', 'puissance', 'prix_neuf']\n        corr_matrix = filtered_df[numeric_cols].corr()\n\n        fig3 = px.imshow(\n            corr_matrix,\n            title='Matrice de corr\u00e9lation',\n            color_continuous_scale='RdBu_r',\n            aspect='auto'\n        )\n\n        # 4. Histogramme de distribution\n        fig4 = px.histogram(\n            filtered_df,\n            x='taux_decote',\n            nbins=30,\n            title='Distribution du taux de d\u00e9cote'\n        )\n\n        return fig1, fig2, fig3, fig4\n\n    return app\n\n# Lancement du dashboard (optionnel)\n# dashboard_app = create_interactive_dashboard()\n# dashboard_app.run_server(debug=True, port=8051)\n</code></pre> <p>Cette analyse exploratoire r\u00e9v\u00e8le les patterns essentiels dans les donn\u00e9es de v\u00e9hicules d'occasion et guide les d\u00e9cisions pour le d\u00e9veloppement du mod\u00e8le pr\u00e9dictif de valeur r\u00e9siduelle.</p>"},{"location":"data/preprocessing/","title":"Preprocessing des donn\u00e9es","text":""},{"location":"data/preprocessing/#vue-densemble","title":"Vue d'ensemble","text":"<p>Le preprocessing est une \u00e9tape cruciale qui transforme les donn\u00e9es brutes collect\u00e9es en un dataset propre et optimis\u00e9 pour l'entra\u00eenement du mod\u00e8le de machine learning.</p>"},{"location":"data/preprocessing/#pipeline-de-preprocessing","title":"Pipeline de preprocessing","text":"<pre><code>graph TD\n    A[Donn\u00e9es brutes] --&gt; B[Nettoyage initial]\n    B --&gt; C[D\u00e9duplication]\n    C --&gt; D[Validation des donn\u00e9es]\n    D --&gt; E[Standardisation des formats]\n    E --&gt; F[Gestion des valeurs manquantes]\n    F --&gt; G[Feature Engineering]\n    G --&gt; H[Encodage des variables]\n    H --&gt; I[Normalisation]\n    I --&gt; J[Dataset final]\n\n    B --&gt; B1[Suppression des outliers]\n    B --&gt; B2[Correction des erreurs]\n\n    G --&gt; G1[Variables d\u00e9riv\u00e9es]\n    G --&gt; G2[Agr\u00e9gations]\n    G --&gt; G3[Interactions]</code></pre>"},{"location":"data/preprocessing/#1-nettoyage-initial","title":"1. Nettoyage initial","text":""},{"location":"data/preprocessing/#suppression-des-outliers","title":"Suppression des outliers","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom scipy import stats\n\ndef remove_outliers(df, columns, method='iqr', threshold=1.5):\n    \"\"\"\n    Suppression des outliers bas\u00e9e sur IQR ou Z-score\n\n    Parameters:\n    - df: DataFrame source\n    - columns: liste des colonnes \u00e0 nettoyer\n    - method: 'iqr' ou 'zscore'\n    - threshold: seuil de d\u00e9tection\n    \"\"\"\n    df_clean = df.copy()\n\n    for col in columns:\n        if method == 'iqr':\n            Q1 = df_clean[col].quantile(0.25)\n            Q3 = df_clean[col].quantile(0.75)\n            IQR = Q3 - Q1\n\n            lower_bound = Q1 - threshold * IQR\n            upper_bound = Q3 + threshold * IQR\n\n            mask = (df_clean[col] &gt;= lower_bound) &amp; (df_clean[col] &lt;= upper_bound)\n\n        elif method == 'zscore':\n            z_scores = np.abs(stats.zscore(df_clean[col]))\n            mask = z_scores &lt; threshold\n\n        df_clean = df_clean[mask]\n\n        print(f\"{col}: {len(df) - len(df_clean)} outliers supprim\u00e9s\")\n\n    return df_clean\n\n# Application sur les donn\u00e9es de prix\ndf_clean = remove_outliers(\n    df_raw, \n    columns=['prix', 'kilometrage', 'puissance'],\n    method='iqr',\n    threshold=2.0\n)\n</code></pre>"},{"location":"data/preprocessing/#correction-des-erreurs-de-saisie","title":"Correction des erreurs de saisie","text":"<pre><code>def correct_data_errors(df):\n    \"\"\"Correction des erreurs communes dans les donn\u00e9es\"\"\"\n\n    df_corrected = df.copy()\n\n    # Correction des marques (normalisation)\n    brand_mapping = {\n        'bmw': 'BMW',\n        'Bmw': 'BMW',\n        'mercedes': 'Mercedes',\n        'Mercedes-Benz': 'Mercedes',\n        'volkswagen': 'Volkswagen',\n        'VW': 'Volkswagen',\n        'renault': 'Renault',\n        'peugeot': 'Peugeot'\n    }\n\n    df_corrected['marque'] = df_corrected['marque'].replace(brand_mapping)\n\n    # Correction des types de carburant\n    fuel_mapping = {\n        'diesel': 'Diesel',\n        'essence': 'Essence',\n        'hybride': 'Hybride',\n        'electrique': 'Electrique',\n        'electric': 'Electrique',\n        'hybrid': 'Hybride'\n    }\n\n    df_corrected['carburant'] = df_corrected['carburant'].replace(fuel_mapping)\n\n    # Correction des incoh\u00e9rences kilom\u00e9trage/\u00e2ge\n    df_corrected['age_vehicule'] = 2024 - df_corrected['annee']\n\n    # V\u00e9hicules avec kilom\u00e9trage impossible (&gt; 50k km/an)\n    max_km_realistic = df_corrected['age_vehicule'] * 50000\n    mask_realistic_km = df_corrected['kilometrage'] &lt;= max_km_realistic\n\n    print(f\"V\u00e9hicules avec kilom\u00e9trage irr\u00e9aliste: {(~mask_realistic_km).sum()}\")\n    df_corrected = df_corrected[mask_realistic_km]\n\n    return df_corrected\n</code></pre>"},{"location":"data/preprocessing/#2-deduplication","title":"2. D\u00e9duplication","text":""},{"location":"data/preprocessing/#identification-des-doublons","title":"Identification des doublons","text":"<pre><code>def identify_duplicates(df, similarity_threshold=0.9):\n    \"\"\"\n    Identification des doublons bas\u00e9e sur la similarit\u00e9\n    des caract\u00e9ristiques principales\n    \"\"\"\n\n    # Colonnes principales pour l'identification\n    key_columns = ['marque', 'modele', 'annee', 'carburant', 'puissance']\n\n    # Doublons exacts\n    exact_duplicates = df.duplicated(subset=key_columns, keep='first')\n    print(f\"Doublons exacts: {exact_duplicates.sum()}\")\n\n    # Doublons approximatifs (kilom\u00e9trage similaire \u00b12000 km)\n    df_no_exact_dups = df[~exact_duplicates].copy()\n\n    # Groupage par caract\u00e9ristiques principales\n    grouped = df_no_exact_dups.groupby(key_columns)\n\n    approximate_duplicates = []\n\n    for name, group in grouped:\n        if len(group) &gt; 1:\n            # Calcul de la matrice de distance pour le kilom\u00e9trage\n            km_values = group['kilometrage'].values\n            km_diff_matrix = np.abs(km_values[:, np.newaxis] - km_values)\n\n            # Indices des v\u00e9hicules avec kilom\u00e9trage similaire\n            similar_pairs = np.where((km_diff_matrix &lt; 2000) &amp; (km_diff_matrix &gt; 0))\n\n            if len(similar_pairs[0]) &gt; 0:\n                # Garder le v\u00e9hicule avec le plus de donn\u00e9es compl\u00e8tes\n                completeness_score = group.notna().sum(axis=1)\n                best_idx = completeness_score.idxmax()\n\n                # Marquer les autres comme doublons\n                duplicates_idx = group.index[group.index != best_idx]\n                approximate_duplicates.extend(duplicates_idx)\n\n    print(f\"Doublons approximatifs: {len(approximate_duplicates)}\")\n\n    return exact_duplicates, approximate_duplicates\n\n# Suppression des doublons\nexact_dups, approx_dups = identify_duplicates(df_corrected)\nall_duplicates = exact_dups | df_corrected.index.isin(approx_dups)\ndf_dedup = df_corrected[~all_duplicates].copy()\n</code></pre>"},{"location":"data/preprocessing/#3-gestion-des-valeurs-manquantes","title":"3. Gestion des valeurs manquantes","text":""},{"location":"data/preprocessing/#analyse-des-valeurs-manquantes","title":"Analyse des valeurs manquantes","text":"<pre><code>def analyze_missing_values(df):\n    \"\"\"Analyse d\u00e9taill\u00e9e des valeurs manquantes\"\"\"\n\n    missing_stats = pd.DataFrame({\n        'column': df.columns,\n        'missing_count': df.isnull().sum(),\n        'missing_percentage': (df.isnull().sum() / len(df)) * 100,\n        'dtype': df.dtypes\n    }).sort_values('missing_percentage', ascending=False)\n\n    print(\"Statistiques des valeurs manquantes:\")\n    print(missing_stats[missing_stats['missing_count'] &gt; 0])\n\n    # Visualisation\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    plt.figure(figsize=(12, 6))\n    missing_data = missing_stats[missing_stats['missing_count'] &gt; 0]\n\n    sns.barplot(data=missing_data, x='missing_percentage', y='column')\n    plt.title('Pourcentage de valeurs manquantes par colonne')\n    plt.xlabel('Pourcentage manquant (%)')\n    plt.tight_layout()\n    plt.show()\n\n    return missing_stats\n\n# Strat\u00e9gies d'imputation\ndef impute_missing_values(df):\n    \"\"\"Imputation des valeurs manquantes avec diff\u00e9rentes strat\u00e9gies\"\"\"\n\n    df_imputed = df.copy()\n\n    # 1. Variables cat\u00e9gorielles - Mode ou 'Inconnu'\n    categorical_columns = ['carburant', 'transmission', 'couleur']\n\n    for col in categorical_columns:\n        if col in df_imputed.columns:\n            if df_imputed[col].isnull().sum() &gt; 0:\n                # Si &lt; 5% manquant: mode, sinon 'Inconnu'\n                missing_pct = df_imputed[col].isnull().sum() / len(df_imputed)\n\n                if missing_pct &lt; 0.05:\n                    mode_value = df_imputed[col].mode()[0]\n                    df_imputed[col].fillna(mode_value, inplace=True)\n                else:\n                    df_imputed[col].fillna('Inconnu', inplace=True)\n\n    # 2. Variables num\u00e9riques - R\u00e9gression ou m\u00e9diane group\u00e9e\n    numeric_columns = ['puissance', 'cylindree', 'consommation']\n\n    for col in numeric_columns:\n        if col in df_imputed.columns and df_imputed[col].isnull().sum() &gt; 0:\n\n            # Imputation par m\u00e9diane group\u00e9e (marque + carburant)\n            for (marque, carburant), group in df_imputed.groupby(['marque', 'carburant']):\n                mask = (df_imputed['marque'] == marque) &amp; \\\n                       (df_imputed['carburant'] == carburant) &amp; \\\n                       df_imputed[col].isnull()\n\n                if mask.sum() &gt; 0 and group[col].notna().sum() &gt; 0:\n                    median_value = group[col].median()\n                    df_imputed.loc[mask, col] = median_value\n\n            # Imputation restante par m\u00e9diane globale\n            remaining_missing = df_imputed[col].isnull().sum()\n            if remaining_missing &gt; 0:\n                global_median = df_imputed[col].median()\n                df_imputed[col].fillna(global_median, inplace=True)\n\n    # 3. Prix neuf - Imputation bas\u00e9e sur le mod\u00e8le\n    if 'prix_neuf' in df_imputed.columns:\n        missing_prix_neuf = df_imputed['prix_neuf'].isnull()\n\n        if missing_prix_neuf.sum() &gt; 0:\n            # Utilisation d'un mod\u00e8le de r\u00e9gression simple\n            from sklearn.ensemble import RandomForestRegressor\n            from sklearn.preprocessing import LabelEncoder\n\n            # Pr\u00e9paration des donn\u00e9es pour l'imputation\n            features_for_imputation = ['marque', 'modele', 'annee', 'puissance', 'cylindree']\n\n            # Donn\u00e9es compl\u00e8tes pour l'entra\u00eenement\n            train_mask = df_imputed['prix_neuf'].notna()\n            X_train = df_imputed[train_mask][features_for_imputation].copy()\n            y_train = df_imputed[train_mask]['prix_neuf']\n\n            # Encodage des variables cat\u00e9gorielles\n            label_encoders = {}\n            for col in ['marque', 'modele']:\n                le = LabelEncoder()\n                X_train[col] = le.fit_transform(X_train[col].astype(str))\n                label_encoders[col] = le\n\n            # Entra\u00eenement du mod\u00e8le d'imputation\n            imputation_model = RandomForestRegressor(n_estimators=100, random_state=42)\n            imputation_model.fit(X_train, y_train)\n\n            # Pr\u00e9diction pour les valeurs manquantes\n            X_missing = df_imputed[missing_prix_neuf][features_for_imputation].copy()\n\n            for col in ['marque', 'modele']:\n                # Gestion des nouvelles cat\u00e9gories\n                le = label_encoders[col]\n                X_missing[col] = X_missing[col].astype(str)\n\n                # Encodage avec gestion des valeurs inconnues\n                unknown_mask = ~X_missing[col].isin(le.classes_)\n                X_missing.loc[unknown_mask, col] = le.classes_[0]  # Valeur par d\u00e9faut\n                X_missing[col] = le.transform(X_missing[col])\n\n            # Pr\u00e9diction\n            predicted_prices = imputation_model.predict(X_missing)\n            df_imputed.loc[missing_prix_neuf, 'prix_neuf'] = predicted_prices\n\n    return df_imputed\n</code></pre>"},{"location":"data/preprocessing/#4-feature-engineering","title":"4. Feature Engineering","text":""},{"location":"data/preprocessing/#creation-de-variables-derivees","title":"Cr\u00e9ation de variables d\u00e9riv\u00e9es","text":"<pre><code>def create_derived_features(df):\n    \"\"\"Cr\u00e9ation de nouvelles variables \u00e0 partir des existantes\"\"\"\n\n    df_features = df.copy()\n\n    # 1. Variables temporelles\n    df_features['age_vehicule'] = 2024 - df_features['annee']\n    df_features['km_par_an'] = df_features['kilometrage'] / df_features['age_vehicule']\n\n    # Gestion division par z\u00e9ro (v\u00e9hicules neufs)\n    df_features['km_par_an'] = df_features['km_par_an'].replace([np.inf, -np.inf], df_features['kilometrage'])\n\n    # 2. Variables de performance\n    df_features['puissance_specifique'] = df_features['puissance'] / df_features['cylindree'] * 1000\n    df_features['ratio_puissance_poids'] = df_features['puissance'] / 1500  # Poids moyen estim\u00e9\n\n    # 3. Segmentation v\u00e9hicule\n    def categorize_vehicle_segment(row):\n        prix_neuf = row['prix_neuf']\n        puissance = row['puissance']\n\n        if prix_neuf &lt; 20000:\n            return 'Economique'\n        elif prix_neuf &lt; 35000:\n            if puissance &gt; 200:\n                return 'Sportif'\n            else:\n                return 'Compact'\n        elif prix_neuf &lt; 50000:\n            return 'Familial'\n        else:\n            return 'Premium'\n\n    df_features['segment'] = df_features.apply(categorize_vehicle_segment, axis=1)\n\n    # 4. Variables de march\u00e9\n    # Popularit\u00e9 de la marque (fr\u00e9quence dans le dataset)\n    brand_counts = df_features['marque'].value_counts()\n    df_features['popularite_marque'] = df_features['marque'].map(brand_counts)\n\n    # Prix moyen de la marque\n    brand_avg_price = df_features.groupby('marque')['prix_neuf'].mean()\n    df_features['prix_moyen_marque'] = df_features['marque'].map(brand_avg_price)\n\n    # 5. Variables d'usage\n    def categorize_usage(km_per_year):\n        if km_per_year &lt; 10000:\n            return 'Faible'\n        elif km_per_year &lt; 20000:\n            return 'Normal'\n        elif km_per_year &lt; 30000:\n            return 'Intensif'\n        else:\n            return 'Tr\u00e8s_intensif'\n\n    df_features['usage_intensite'] = df_features['km_par_an'].apply(categorize_usage)\n\n    # 6. Interactions importantes\n    df_features['age_x_km'] = df_features['age_vehicule'] * df_features['kilometrage']\n    df_features['puissance_x_age'] = df_features['puissance'] * df_features['age_vehicule']\n\n    # 7. Variables cycliques (pour l'\u00e2ge)\n    df_features['age_sin'] = np.sin(2 * np.pi * df_features['age_vehicule'] / 20)  # Cycle 20 ans\n    df_features['age_cos'] = np.cos(2 * np.pi * df_features['age_vehicule'] / 20)\n\n    return df_features\n\n# Application du feature engineering\ndf_with_features = create_derived_features(df_imputed)\n</code></pre>"},{"location":"data/preprocessing/#variables-de-contexte-economique","title":"Variables de contexte \u00e9conomique","text":"<pre><code>def add_economic_context(df):\n    \"\"\"Ajout de variables de contexte \u00e9conomique\"\"\"\n\n    # Indices \u00e9conomiques par ann\u00e9e (donn\u00e9es simul\u00e9es - \u00e0 remplacer par de vraies donn\u00e9es)\n    economic_indices = {\n        2017: {'inflation': 1.0, 'pib_growth': 2.3, 'unemployment': 9.4},\n        2018: {'inflation': 2.1, 'pib_growth': 1.8, 'unemployment': 9.1},\n        2019: {'inflation': 1.1, 'pib_growth': 1.5, 'unemployment': 8.5},\n        2020: {'inflation': 0.5, 'pib_growth': -7.9, 'unemployment': 8.0},\n        2021: {'inflation': 1.6, 'pib_growth': 7.0, 'unemployment': 7.9},\n        2022: {'inflation': 5.2, 'pib_growth': 2.6, 'unemployment': 7.3},\n        2023: {'inflation': 4.9, 'pib_growth': 0.9, 'unemployment': 7.2},\n        2024: {'inflation': 2.3, 'pib_growth': 1.3, 'unemployment': 7.5}\n    }\n\n    df_context = df.copy()\n\n    # Ajout des indices \u00e9conomiques\n    for year, indices in economic_indices.items():\n        mask = df_context['annee'] == year\n        for indicator, value in indices.items():\n            df_context.loc[mask, f'{indicator}_annee_achat'] = value\n\n    # Variables de saisonnalit\u00e9 (si date de collecte disponible)\n    if 'date_collecte' in df_context.columns:\n        df_context['mois_collecte'] = pd.to_datetime(df_context['date_collecte']).dt.month\n        df_context['trimestre_collecte'] = pd.to_datetime(df_context['date_collecte']).dt.quarter\n\n        # Saisonnalit\u00e9 automobile (les ventes varient selon les mois)\n        seasonal_factors = {\n            1: 0.85, 2: 0.88, 3: 1.05, 4: 1.02,  # Q1\n            5: 1.08, 6: 1.12, 7: 0.95, 8: 0.92,  # Q2-Q3\n            9: 1.15, 10: 1.18, 11: 1.03, 12: 0.97  # Q4\n        }\n\n        df_context['facteur_saisonnier'] = df_context['mois_collecte'].map(seasonal_factors)\n\n    return df_context\n</code></pre>"},{"location":"data/preprocessing/#5-encodage-des-variables","title":"5. Encodage des variables","text":""},{"location":"data/preprocessing/#encodage-des-variables-categorielles","title":"Encodage des variables cat\u00e9gorielles","text":"<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport category_encoders as ce\n\ndef encode_categorical_variables(df, target_column='taux_decote'):\n    \"\"\"Encodage optimis\u00e9 des variables cat\u00e9gorielles\"\"\"\n\n    df_encoded = df.copy()\n\n    # 1. Variables avec peu de modalit\u00e9s - One-Hot Encoding\n    low_cardinality_cols = ['carburant', 'transmission', 'segment', 'usage_intensite']\n\n    for col in low_cardinality_cols:\n        if col in df_encoded.columns:\n            n_categories = df_encoded[col].nunique()\n\n            if n_categories &lt;= 10:  # Seuil pour One-Hot\n                # One-Hot Encoding avec pr\u00e9fixe\n                dummies = pd.get_dummies(df_encoded[col], prefix=col)\n                df_encoded = pd.concat([df_encoded, dummies], axis=1)\n                df_encoded.drop(col, axis=1, inplace=True)\n\n    # 2. Variables avec beaucoup de modalit\u00e9s - Target Encoding\n    high_cardinality_cols = ['marque', 'modele']\n\n    for col in high_cardinality_cols:\n        if col in df_encoded.columns:\n            n_categories = df_encoded[col].nunique()\n\n            if n_categories &gt; 10:\n                # Target Encoding (moyenne du target par cat\u00e9gorie)\n                target_encoder = ce.TargetEncoder(cols=[col])\n\n                # \u00c9viter le data leakage avec validation crois\u00e9e\n                from sklearn.model_selection import KFold\n\n                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n                encoded_values = np.zeros(len(df_encoded))\n\n                for train_idx, val_idx in kf.split(df_encoded):\n                    # Entra\u00eenement sur le fold de train\n                    encoder_fold = ce.TargetEncoder(cols=[col])\n                    encoder_fold.fit(df_encoded.iloc[train_idx][[col]], \n                                   df_encoded.iloc[train_idx][target_column])\n\n                    # Transformation du fold de validation\n                    encoded_val = encoder_fold.transform(df_encoded.iloc[val_idx][[col]])\n                    encoded_values[val_idx] = encoded_val[col].values\n\n                # Remplacement de la colonne originale\n                df_encoded[f'{col}_encoded'] = encoded_values\n                df_encoded.drop(col, axis=1, inplace=True)\n\n    # 3. Variables ordinales - Label Encoding ordonn\u00e9\n    ordinal_mappings = {\n        'etat_vehicule': {'Excellent': 4, 'Tr\u00e8s bon': 3, 'Bon': 2, 'Correct': 1, 'Mauvais': 0}\n    }\n\n    for col, mapping in ordinal_mappings.items():\n        if col in df_encoded.columns:\n            df_encoded[col] = df_encoded[col].map(mapping)\n\n    return df_encoded\n\n# Sauvegarde des encodeurs pour la production\ndef save_encoders(encoders, filepath='models/encoders.joblib'):\n    \"\"\"Sauvegarde des encodeurs pour utilisation en production\"\"\"\n    import joblib\n    joblib.dump(encoders, filepath)\n    print(f\"Encodeurs sauvegard\u00e9s dans {filepath}\")\n</code></pre>"},{"location":"data/preprocessing/#6-pipeline-de-preprocessing-complet","title":"6. Pipeline de preprocessing complet","text":"<pre><code>from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\ndef create_preprocessing_pipeline():\n    \"\"\"Cr\u00e9ation du pipeline de preprocessing complet\"\"\"\n\n    # Identification des types de colonnes\n    numeric_features = [\n        'kilometrage', 'puissance', 'cylindree', 'age_vehicule', \n        'km_par_an', 'puissance_specifique', 'popularite_marque'\n    ]\n\n    categorical_features = [\n        'carburant', 'transmission', 'segment', 'usage_intensite'\n    ]\n\n    high_cardinality_features = ['marque_encoded', 'modele_encoded']\n\n    # Transformateurs pour chaque type\n    numeric_transformer = Pipeline(steps=[\n        ('scaler', RobustScaler())  # Robuste aux outliers\n    ])\n\n    categorical_transformer = Pipeline(steps=[\n        ('onehot', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))\n    ])\n\n    # Pipeline principal\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features),\n            ('high_card', 'passthrough', high_cardinality_features)\n        ],\n        remainder='drop'  # Supprime les colonnes non sp\u00e9cifi\u00e9es\n    )\n\n    return preprocessor\n\n# Application du pipeline\ndef apply_complete_preprocessing(df_raw):\n    \"\"\"Application compl\u00e8te du preprocessing\"\"\"\n\n    print(\"1. Nettoyage initial...\")\n    df_clean = correct_data_errors(df_raw)\n    df_clean = remove_outliers(df_clean, ['prix', 'kilometrage', 'puissance'])\n\n    print(\"2. D\u00e9duplication...\")\n    exact_dups, approx_dups = identify_duplicates(df_clean)\n    all_duplicates = exact_dups | df_clean.index.isin(approx_dups)\n    df_dedup = df_clean[~all_duplicates].copy()\n\n    print(\"3. Gestion des valeurs manquantes...\")\n    df_imputed = impute_missing_values(df_dedup)\n\n    print(\"4. Feature engineering...\")\n    df_features = create_derived_features(df_imputed)\n    df_features = add_economic_context(df_features)\n\n    print(\"5. Calcul de la variable cible...\")\n    df_features['taux_decote'] = df_features['prix'] / df_features['prix_neuf']\n\n    print(\"6. Encodage des variables...\")\n    df_encoded = encode_categorical_variables(df_features)\n\n    print(\"7. Validation finale...\")\n    # Suppression des lignes avec des valeurs impossibles\n    valid_mask = (\n        (df_encoded['taux_decote'] &gt; 0.1) &amp; \n        (df_encoded['taux_decote'] &lt; 1.0) &amp;\n        (df_encoded['age_vehicule'] &gt;= 0) &amp;\n        (df_encoded['kilometrage'] &gt;= 0)\n    )\n\n    df_final = df_encoded[valid_mask].copy()\n\n    print(f\"Dataset final: {len(df_final)} v\u00e9hicules\")\n    print(f\"R\u00e9duction: {len(df_raw) - len(df_final)} v\u00e9hicules supprim\u00e9s ({(len(df_raw) - len(df_final))/len(df_raw)*100:.1f}%)\")\n\n    return df_final\n\n# Sauvegarde du dataset preprocess\u00e9\ndef save_processed_data(df, filepath='data/processed_data/preprocessed_data.csv'):\n    \"\"\"Sauvegarde du dataset preprocess\u00e9\"\"\"\n    df.to_csv(filepath, index=False)\n    print(f\"Dataset sauvegard\u00e9: {filepath}\")\n\n    # Statistiques r\u00e9sum\u00e9es\n    print(\"\\nStatistiques du dataset final:\")\n    print(f\"- Nombre de v\u00e9hicules: {len(df):,}\")\n    print(f\"- Variables: {len(df.columns)}\")\n    print(f\"- Taux de d\u00e9cote moyen: {df['taux_decote'].mean():.3f}\")\n    print(f\"- \u00c9cart-type: {df['taux_decote'].std():.3f}\")\n</code></pre> <p>Ce pipeline de preprocessing garantit la qualit\u00e9 et la coh\u00e9rence des donn\u00e9es pour l'entra\u00eenement du mod\u00e8le, tout en pr\u00e9servant l'information utile et en cr\u00e9ant des variables pertinentes pour la pr\u00e9diction de la valeur r\u00e9siduelle.</p>"},{"location":"data/sources/","title":"Sources de donn\u00e9es","text":""},{"location":"data/sources/#vue-densemble","title":"Vue d'ensemble","text":"<p>Le projet utilise deux sources principales de donn\u00e9es pour construire un dataset complet permettant le calcul de la valeur r\u00e9siduelle des v\u00e9hicules.</p>"},{"location":"data/sources/#source-1-annonces-vehicules-doccasion","title":"Source 1 : Annonces v\u00e9hicules d'occasion","text":""},{"location":"data/sources/#autoherocom","title":"autohero.com","text":"<ul> <li>Type : Site de vente de v\u00e9hicules d'occasion</li> <li>M\u00e9thode : Web scraping avec Selenium</li> <li>Donn\u00e9es collect\u00e9es :</li> <li>Prix de vente</li> <li>Marque et mod\u00e8le</li> <li>Ann\u00e9e de mise en circulation</li> <li>Kilom\u00e9trage</li> <li>Carburant</li> <li>Transmission</li> <li>Puissance</li> <li>\u00c9tat du v\u00e9hicule</li> </ul>"},{"location":"data/sources/#criteres-de-selection","title":"Crit\u00e8res de s\u00e9lection","text":"<pre><code># Filtres appliqu\u00e9s lors du scraping\nANNEE_MIN = 2017\nKILOMETRAGE_MAX = 100000\n\n# Justification des crit\u00e8res\n</code></pre> <p>Justification des crit\u00e8res</p> <ul> <li>Ann\u00e9e \u2265 2017 : V\u00e9hicules r\u00e9cents </li> <li>Kilom\u00e9trage \u2264 100 000 km : V\u00e9hicules en bon \u00e9tat g\u00e9n\u00e9ral</li> <li>Focus sur la qualit\u00e9 : Donn\u00e9es plus fiables et repr\u00e9sentatives</li> </ul>"},{"location":"data/sources/#structure-des-donnees-brutes","title":"Structure des donn\u00e9es brutes","text":"Colonne Type Description Exemple <code>prix</code> float Prix affich\u00e9 (\u20ac) 25000.0 <code>marque</code> string Marque du v\u00e9hicule \"BMW\" <code>modele</code> string Mod\u00e8le du v\u00e9hicule \"S\u00e9rie 3\" <code>annee</code> int Ann\u00e9e de mise en circulation 2019 <code>kilometrage</code> int Kilom\u00e9trage (km) 45000 <code>carburant</code> string Type de carburant \"Essence\" <code>transmission</code> string Type de transmission \"Automatique\" <code>puissance</code> int Puissance (CV) 150"},{"location":"data/sources/#source-2-fiches-techniques-et-prix-neufs","title":"Source 2 : Fiches techniques et prix neufs","text":""},{"location":"data/sources/#caradisiaccom","title":"caradisiac.com","text":"<ul> <li>Type : Site de fiches techniques automobiles</li> <li>M\u00e9thode : Web scraping cibl\u00e9 avec Selenium</li> <li>Donn\u00e9es collect\u00e9es :</li> <li>Prix catalogue neuf</li> <li>Caract\u00e9ristiques techniques d\u00e9taill\u00e9es</li> <li>\u00c9quipements de s\u00e9rie</li> <li>Consommation officielle</li> </ul>"},{"location":"data/sources/#donnees-de-reference","title":"Donn\u00e9es de r\u00e9f\u00e9rence","text":"Colonne Type Description Exemple <code>prix_neuf</code> float Prix catalogue neuf (\u20ac) 35000.0 <code>consommation</code> float Consommation mixte (L/100km) 6.2 <code>emissions_co2</code> int \u00c9missions CO2 (g/km) 142 <code>cylindree</code> int Cylindr\u00e9e (cm\u00b3) 1998 <code>nb_places</code> int Nombre de places 5"},{"location":"data/sources/#processus-de-collecte","title":"Processus de collecte","text":""},{"location":"data/sources/#1-scraping-des-annonces","title":"1. Scraping des annonces","text":"<pre><code># Configuration Selenium\ndriver_options = webdriver.ChromeOptions()\ndriver_options.add_argument('--headless')\ndriver_options.add_argument('--no-sandbox')\n\n# Exemple de script de scraping\ndef scrape_autohero():\n    # Initialisation du driver\n    # Navigation et collecte\n    # Sauvegarde en CSV\n    pass\n</code></pre>"},{"location":"data/sources/#2-collecte-des-prix-neufs","title":"2. Collecte des prix neufs","text":"<pre><code>def scrape_prix_neuf():\n    # Mapping marque/mod\u00e8le\n    # Requ\u00eates vers fiches techniques\n    # Extraction prix et caract\u00e9ristiques\n    pass\n</code></pre>"},{"location":"data/sources/#qualite-des-donnees","title":"Qualit\u00e9 des donn\u00e9es","text":""},{"location":"data/sources/#statistiques-de-collecte","title":"Statistiques de collecte","text":"M\u00e9trique Valeur Nombre d'annonces collect\u00e9es 2352 P\u00e9riode de collecte 2025-04-09 Couverture marques 34 marques Couverture mod\u00e8les 221 mod\u00e8les"},{"location":"data/sources/#problemes-identifies","title":"Probl\u00e8mes identifi\u00e9s","text":"<p>Points d'attention</p> <ul> <li>Doublons : M\u00eame v\u00e9hicule sur plusieurs annonces</li> <li>Donn\u00e9es manquantes : Certains champs non renseign\u00e9s</li> <li>Incoh\u00e9rences : Prix annonc\u00e9 &gt; Prix neuf sur quelques annonces</li> </ul>"},{"location":"data/sources/#solutions-mises-en-place","title":"Solutions mises en place","text":"<pre><code># D\u00e9duplication\ndf.drop_duplicates(subset=['marque', 'modele', 'annee', 'kilometrage'], keep='first')\n\n# Validation des donn\u00e9es\ndef validate_data(df):\n    # Prix coh\u00e9rents\n    df = df[(df['prix'] &gt; 5000) &amp; (df['prix'] &lt; 100000)]\n    # Kilom\u00e9trage r\u00e9aliste\n    df = df[df['kilometrage'] &lt; (2024 - df['annee']) * 25000]\n    return df\n</code></pre>"},{"location":"data/sources/#organisation-des-fichiers","title":"Organisation des fichiers","text":"<pre><code>data/\n\u251c\u2500\u2500 raw_data/\n\u2502   \u251c\u2500\u2500 autohero.csv                 # Donn\u00e9es brutes autohero\n\u2502   \u251c\u2500\u2500 prix_neuf_voitures_*.csv    # Donn\u00e9es prix neufs par batch\n\u2502   \u2514\u2500\u2500 *_error.csv                 # Logs d'erreurs\n\u251c\u2500\u2500 processed_data/\n\u2502   \u251c\u2500\u2500 preprocessed_data.csv       # Dataset final nettoy\u00e9\n\u2502   \u2514\u2500\u2500 modeles_voitures.csv        # R\u00e9f\u00e9rentiel mod\u00e8les\n</code></pre>"},{"location":"data/sources/#mise-a-jour-des-donnees-a-implementer-en-cible","title":"Mise \u00e0 jour des donn\u00e9es (\u00e0 impl\u00e9menter en cible)","text":""},{"location":"data/sources/#frequence","title":"Fr\u00e9quence","text":"<ul> <li>Donn\u00e9es d'occasion : Scraping trimestriel</li> <li>Prix neufs : Mise \u00e0 jour trimestrielle</li> <li>Validation : Contr\u00f4les automatis\u00e9s</li> </ul>"},{"location":"data/sources/#pipeline-automatise","title":"Pipeline automatis\u00e9","text":"<ol> <li>D\u00e9clenchement programm\u00e9</li> <li>Scraping avec gestion d'erreurs</li> <li>Validation et nettoyage</li> <li>Int\u00e9gration au dataset principal</li> <li>R\u00e9-entra\u00eenement du mod\u00e8le si n\u00e9cessaire</li> </ol>"},{"location":"dev/build-html/","title":"G\u00e9n\u00e9ration HTML statique - Guide complet","text":""},{"location":"dev/build-html/#transformer-mkdocs-en-html-statique","title":"\ud83c\udf10 Transformer MkDocs en HTML statique","text":""},{"location":"dev/build-html/#pourquoi-generer-du-html-statique","title":"Pourquoi g\u00e9n\u00e9rer du HTML statique ?","text":"<ul> <li>\u2705 Pas besoin de serveur : Fichiers consultables directement</li> <li>\u2705 H\u00e9bergement simple : Sur n'importe quel serveur web</li> <li>\u2705 Partage facile : Envoi par email ou upload sur cloud</li> <li>\u2705 Performance : Chargement rapide, pas de latence serveur</li> </ul>"},{"location":"dev/build-html/#processus-de-construction","title":"Processus de construction","text":"<pre><code># Construction de la documentation\nmkdocs build\n\n# R\u00e9sultat : dossier 'site/' contenant tout le HTML\n</code></pre>"},{"location":"dev/build-html/#structure-generee","title":"Structure g\u00e9n\u00e9r\u00e9e","text":"<p>Apr\u00e8s <code>mkdocs build</code>, vous obtenez :</p> <pre><code>site/\n\u251c\u2500\u2500 index.html                    # Page d'accueil\n\u251c\u2500\u2500 assets/                       # CSS, JS, images\n\u2502   \u251c\u2500\u2500 stylesheets/\n\u2502   \u251c\u2500\u2500 javascripts/\n\u2502   \u2514\u2500\u2500 images/\n\u251c\u2500\u2500 projet/\n\u2502   \u2514\u2500\u2500 overview/\n\u2502       \u2514\u2500\u2500 index.html\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 sources/\n\u2502   \u251c\u2500\u2500 preprocessing/\n\u2502   \u2514\u2500\u2500 eda/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 modeling/\n\u2502   \u251c\u2500\u2500 evaluation/\n\u2502   \u2514\u2500\u2500 performance/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 interface/\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2514\u2500\u2500 deployment/\n\u251c\u2500\u2500 dev/\n\u2502   \u2514\u2500\u2500 installation/\n\u251c\u2500\u2500 notebooks/\n\u2502   \u2514\u2500\u2500 VR_PoC_modelling/\n\u251c\u2500\u2500 search/\n\u2502   \u2514\u2500\u2500 search_index.json         # Index de recherche\n\u2514\u2500\u2500 sitemap.xml                   # Plan du site\n\n# Tous les fichiers n\u00e9cessaires pour fonctionner offline !\n</code></pre>"},{"location":"dev/build-html/#instructions-etape-par-etape","title":"\ud83d\udccb Instructions \u00e9tape par \u00e9tape","text":""},{"location":"dev/build-html/#etape-1-preparer-lenvironnement","title":"\u00c9tape 1 : Pr\u00e9parer l'environnement","text":"<pre><code># 1. Aller dans le dossier du projet\ncd \"c:\\Users\\Hong-CuongLE\\OneDrive - NEXIALOG\\Documents\\Valeur-Residuelle\"\n\n# 2. Activer l'environnement virtuel\n.\\.venv\\Scripts\\Activate.ps1\n\n# 3. V\u00e9rifier que MkDocs est install\u00e9\nmkdocs --version\n</code></pre>"},{"location":"dev/build-html/#etape-2-construire-la-documentation","title":"\u00c9tape 2 : Construire la documentation","text":"<pre><code># Construction simple\nmkdocs build\n\n# Construction avec nettoyage pr\u00e9alable (recommand\u00e9)\nmkdocs build --clean\n\n# Construction en mode verbose pour voir les d\u00e9tails\nmkdocs build --verbose\n</code></pre>"},{"location":"dev/build-html/#etape-3-verifier-le-resultat","title":"\u00c9tape 3 : V\u00e9rifier le r\u00e9sultat","text":"<pre><code># V\u00e9rifier que le dossier 'site' a \u00e9t\u00e9 cr\u00e9\u00e9\nls site/\n\n# Compter les fichiers g\u00e9n\u00e9r\u00e9s\n(Get-ChildItem -Recurse site\\).Count\n</code></pre>"},{"location":"dev/build-html/#etape-4-tester-la-documentation","title":"\u00c9tape 4 : Tester la documentation","text":"<pre><code># Ouvrir directement dans le navigateur\nstart site\\index.html\n\n# Ou utiliser un serveur web simple pour tester\npython -m http.server 8000 --directory site\n# Puis aller sur http://localhost:8000\n</code></pre>"},{"location":"dev/build-html/#options-de-deploiement","title":"\ud83d\ude80 Options de d\u00e9ploiement","text":""},{"location":"dev/build-html/#option-1-serveur-web-local","title":"Option 1 : Serveur web local","text":"<pre><code># Serveur Python simple\ncd site\npython -m http.server 8080\n\n# Accessible sur http://localhost:8080\n</code></pre>"},{"location":"dev/build-html/#option-2-hebergement-cloud-gratuit","title":"Option 2 : H\u00e9bergement cloud gratuit","text":""},{"location":"dev/build-html/#github-pages","title":"GitHub Pages","text":"<pre><code># D\u00e9ploiement automatique\nmkdocs gh-deploy\n\n# Accessible sur https://username.github.io/repo-name\n</code></pre>"},{"location":"dev/build-html/#netlify","title":"Netlify","text":"<ol> <li>Zipper le dossier <code>site/</code></li> <li>Faire un drag &amp; drop sur netlify.com</li> <li>Documentation en ligne instantan\u00e9ment !</li> </ol>"},{"location":"dev/build-html/#vercel","title":"Vercel","text":"<pre><code># Installation\nnpm i -g vercel\n\n# D\u00e9ploiement\ncd site\nvercel --prod\n</code></pre>"},{"location":"dev/build-html/#option-3-serveur-interne-dentreprise","title":"Option 3 : Serveur interne d'entreprise","text":"<pre><code># Copier le dossier site/ sur votre serveur web\nrobocopy site\\ \\\\serveur\\web\\documentation\\ /E /PURGE\n</code></pre>"},{"location":"dev/build-html/#personnalisation-avancee","title":"\ud83d\udcc1 Personnalisation avanc\u00e9e","text":""},{"location":"dev/build-html/#configuration-du-build","title":"Configuration du build","text":"<p>Ajoutez dans <code>mkdocs.yml</code> :</p> <pre><code># Configuration de construction\nsite_dir: 'documentation_html'  # Nom du dossier de sortie\nuse_directory_urls: false       # URLs relatives pour offline\n\n# Optimisations\nextra:\n  alternate:\n    - name: Fran\u00e7ais\n      link: ./\n      lang: fr\n  manifest: 'manifest.webmanifest'\n\n# Hook pour post-processing\nhooks:\n  - scripts/build_hook.py\n</code></pre>"},{"location":"dev/build-html/#script-de-construction-automatise","title":"Script de construction automatis\u00e9","text":"<pre><code># Cr\u00e9er un script build.ps1\n@\"\n#!/usr/bin/env powershell\n\nWrite-Host \"\ud83c\udfd7\ufe0f  Construction de la documentation...\" -ForegroundColor Blue\n\n# Nettoyer l'ancien build\nif (Test-Path \"site\") {\n    Remove-Item -Recurse -Force site\n    Write-Host \"\u2705 Ancien build supprim\u00e9\" -ForegroundColor Green\n}\n\n# Construire\nmkdocs build --clean --verbose\n\nif ($LASTEXITCODE -eq 0) {\n    Write-Host \"\u2705 Documentation construite avec succ\u00e8s !\" -ForegroundColor Green\n    Write-Host \"\ud83d\udcc1 Fichiers disponibles dans le dossier 'site/'\" -ForegroundColor Yellow\n\n    # Ouvrir automatiquement\n    $response = Read-Host \"Voulez-vous ouvrir la documentation ? (y/N)\"\n    if ($response -eq \"y\" -or $response -eq \"Y\") {\n        start site\\index.html\n    }\n} else {\n    Write-Host \"\u274c Erreur lors de la construction\" -ForegroundColor Red\n    exit 1\n}\n\"@ | Out-File -FilePath build.ps1 -Encoding UTF8\n\n# Rendre ex\u00e9cutable et lancer\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n.\\build.ps1\n</code></pre>"},{"location":"dev/build-html/#resolution-de-problemes","title":"\ud83d\udd27 R\u00e9solution de probl\u00e8mes","text":""},{"location":"dev/build-html/#probleme-mkdocs-non-trouve","title":"Probl\u00e8me : MkDocs non trouv\u00e9","text":"<pre><code># Solution 1 : V\u00e9rifier l'environnement virtuel\n.\\.venv\\Scripts\\Activate.ps1\npip list | findstr mkdocs\n\n# Solution 2 : R\u00e9installer\npip install --upgrade mkdocs mkdocs-material\n\n# Solution 3 : Utiliser le chemin complet\n.\\.venv\\Scripts\\mkdocs.exe build\n</code></pre>"},{"location":"dev/build-html/#probleme-erreurs-de-build","title":"Probl\u00e8me : Erreurs de build","text":"<pre><code># Debug mode\nmkdocs build --verbose\n\n# V\u00e9rifier la configuration\nmkdocs config\n\n# Tester la configuration\nmkdocs serve --strict\n</code></pre>"},{"location":"dev/build-html/#probleme-ressources-manquantes","title":"Probl\u00e8me : Ressources manquantes","text":"<pre><code># Dans mkdocs.yml, ajouter :\nextra_css:\n  - assets/extra.css\nextra_javascript:\n  - assets/extra.js\n\n# S'assurer que tous les liens sont relatifs\n</code></pre>"},{"location":"dev/build-html/#avantages-de-la-version-html-statique","title":"\ud83d\udcca Avantages de la version HTML statique","text":"Aspect Serveur MkDocs HTML Statique D\u00e9marrage <code>mkdocs serve</code> Ouvrir <code>index.html</code> Performance Latence serveur Instantan\u00e9 H\u00e9bergement Port sp\u00e9cifique N'importe o\u00f9 Partage URL + port Fichiers ZIP Offline Non Oui S\u00e9curit\u00e9 Port expos\u00e9 Fichiers statiques <p>Votre documentation sera 100% autonome et consultable sans serveur !</p>"},{"location":"dev/installation/","title":"Installation et configuration","text":""},{"location":"dev/installation/#prerequis","title":"Pr\u00e9requis","text":""},{"location":"dev/installation/#systeme","title":"Syst\u00e8me","text":"<ul> <li>Python : Version 3.9 ou sup\u00e9rieure</li> <li>Git : Pour le clonage du repository</li> <li>Chrome/Chromium : Requis pour le web scraping</li> </ul>"},{"location":"dev/installation/#outils-recommandes","title":"Outils recommand\u00e9s","text":"<ul> <li>IDE : Visual Studio Code avec extensions Python</li> <li>Terminal : PowerShell (Windows) ou bash (Linux/macOS)</li> <li>Gestionnaire de paquets : pip (inclus avec Python)</li> </ul>"},{"location":"dev/installation/#installation","title":"Installation","text":""},{"location":"dev/installation/#1-cloner-le-repository","title":"1. Cloner le repository","text":"<pre><code>git clone https://github.com/Hcle18/Valeur-Residuelle.git\ncd Valeur-Residuelle\n</code></pre>"},{"location":"dev/installation/#2-creer-un-environnement-virtuel","title":"2. Cr\u00e9er un environnement virtuel","text":"Windows (PowerShell)Linux/macOS <pre><code>python -m venv venv\n.\\venv\\Scripts\\Activate.ps1\n</code></pre> <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"dev/installation/#3-installer-les-dependances","title":"3. Installer les d\u00e9pendances","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"dev/installation/#4-configuration-des-variables-denvironnement","title":"4. Configuration des variables d'environnement","text":"<p>Cr\u00e9er un fichier <code>.env</code> \u00e0 la racine du projet :</p> <pre><code># Configuration base de donn\u00e9es\nDATABASE_URL=sqlite:///instance/car_data.db\n\n# Configuration scraping\nCHROME_DRIVER_PATH=/path/to/chromedriver\nSCRAPING_DELAY=2\n\n# Configuration application\nFLASK_ENV=development\nSECRET_KEY=your-secret-key-here\n</code></pre>"},{"location":"dev/installation/#verification-de-linstallation","title":"V\u00e9rification de l'installation","text":""},{"location":"dev/installation/#test-des-imports-python","title":"Test des imports Python","text":"<pre><code># Test des imports principaux\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport xgboost\nimport dash\n\nprint(\"\u2705 Tous les modules sont install\u00e9s correctement\")\n</code></pre>"},{"location":"dev/installation/#test-de-lapplication","title":"Test de l'application","text":"<pre><code># Lancer l'application de test\npython app.py\n</code></pre> <p>L'application devrait \u00eatre accessible sur <code>http://localhost:8050</code></p>"},{"location":"dev/installation/#structure-du-projet-apres-installation","title":"Structure du projet apr\u00e8s installation","text":"<pre><code>Valeur-Residuelle/\n\u251c\u2500\u2500 app.py                          # Application principale\n\u251c\u2500\u2500 requirements.txt                # D\u00e9pendances Python\n\u251c\u2500\u2500 mkdocs.yml                      # Configuration documentation\n\u251c\u2500\u2500 .env                            # Variables d'environnement (\u00e0 cr\u00e9er)\n\u251c\u2500\u2500 venv/                          # Environnement virtuel (cr\u00e9\u00e9)\n\u251c\u2500\u2500 data/                          # Donn\u00e9es du projet\n\u251c\u2500\u2500 src/                           # Code source\n\u251c\u2500\u2500 models/                        # Mod\u00e8les entra\u00een\u00e9s\n\u251c\u2500\u2500 notebooks/                     # Notebooks Jupyter\n\u251c\u2500\u2500 docs/                          # Documentation\n\u2514\u2500\u2500 tests/                         # Tests unitaires\n</code></pre>"},{"location":"dev/installation/#configuration-des-outils-de-developpement","title":"Configuration des outils de d\u00e9veloppement","text":""},{"location":"dev/installation/#visual-studio-code","title":"Visual Studio Code","text":"<p>Extensions recommand\u00e9es :</p> <pre><code>{\n  \"recommendations\": [\n    \"ms-python.python\",\n    \"ms-python.vscode-pylance\",\n    \"ms-toolsai.jupyter\",\n    \"yzhang.markdown-all-in-one\",\n    \"james-yu.latex-workshop\"\n  ]\n}\n</code></pre>"},{"location":"dev/installation/#jupyter","title":"Jupyter","text":"<p>Pour utiliser les notebooks :</p> <pre><code># Installation si n\u00e9cessaire\npip install jupyter\n\n# Lancement\njupyter notebook\n</code></pre>"},{"location":"dev/installation/#donnees-dexemple","title":"Donn\u00e9es d'exemple","text":""},{"location":"dev/installation/#telecharger-les-donnees-de-test","title":"T\u00e9l\u00e9charger les donn\u00e9es de test","text":"<pre><code># Cr\u00e9er les dossiers de donn\u00e9es\nmkdir -p data/raw_data data/processed_data data/outil_data\n\n# Les donn\u00e9es d'exemple sont incluses dans le repository\n# V\u00e9rifier la pr\u00e9sence des fichiers\nls data/outil_data/sample_app_car_data.csv\n</code></pre>"},{"location":"dev/installation/#initialisation-de-la-base-de-donnees","title":"Initialisation de la base de donn\u00e9es","text":"<pre><code># Script d'initialisation\nfrom src.app.database import init_db\n\n# Cr\u00e9er les tables\ninit_db()\nprint(\"\u2705 Base de donn\u00e9es initialis\u00e9e\")\n</code></pre>"},{"location":"dev/installation/#depannage","title":"D\u00e9pannage","text":""},{"location":"dev/installation/#problemes-courants","title":"Probl\u00e8mes courants","text":""},{"location":"dev/installation/#1-erreur-dimport-de-modules","title":"1. Erreur d'import de modules","text":"<pre><code># V\u00e9rifier l'environnement virtuel\nwhich python  # doit pointer vers venv/bin/python\n\n# R\u00e9installer les d\u00e9pendances\npip install --upgrade -r requirements.txt\n</code></pre>"},{"location":"dev/installation/#2-probleme-avec-chromedriver","title":"2. Probl\u00e8me avec ChromeDriver","text":"<pre><code># Installation automatique\npip install webdriver-manager\n\n# Ou t\u00e9l\u00e9chargement manuel depuis\n# https://chromedriver.chromium.org/\n</code></pre>"},{"location":"dev/installation/#3-erreur-de-permissions","title":"3. Erreur de permissions","text":"WindowsLinux/macOS <pre><code># Modifier la politique d'ex\u00e9cution\nSet-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n</code></pre> <pre><code># V\u00e9rifier les permissions\nchmod +x venv/bin/activate\n</code></pre>"},{"location":"dev/installation/#logs-et-debug","title":"Logs et debug","text":"<p>Activation du mode debug :</p> <pre><code># Dans app.py\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Ou via variable d'environnement\nexport FLASK_ENV=development\n</code></pre>"},{"location":"dev/installation/#mise-a-jour","title":"Mise \u00e0 jour","text":""},{"location":"dev/installation/#mettre-a-jour-les-dependances","title":"Mettre \u00e0 jour les d\u00e9pendances","text":"<pre><code># Sauvegarder l'\u00e9tat actuel\npip freeze &gt; requirements_backup.txt\n\n# Mettre \u00e0 jour\npip install --upgrade -r requirements.txt\n\n# En cas de probl\u00e8me, revenir \u00e0 l'\u00e9tat pr\u00e9c\u00e9dent\npip install -r requirements_backup.txt\n</code></pre>"},{"location":"dev/installation/#mise-a-jour-du-code","title":"Mise \u00e0 jour du code","text":"<pre><code># R\u00e9cup\u00e9rer les derni\u00e8res modifications\ngit pull origin main\n\n# Installer les nouvelles d\u00e9pendances si n\u00e9cessaire\npip install -r requirements.txt\n</code></pre>"},{"location":"dev/installation/#support","title":"Support","text":""},{"location":"dev/installation/#ressources-utiles","title":"Ressources utiles","text":"<ul> <li>Documentation officielle : Cette documentation MkDocs</li> <li>Repository GitHub : Valeur-Residuelle</li> <li>Issues GitHub : Pour signaler des bugs ou demander des fonctionnalit\u00e9s</li> </ul>"},{"location":"dev/installation/#contact","title":"Contact","text":"<p>Pour toute question technique : - Cr\u00e9er une issue sur GitHub - Contacter l'\u00e9quipe NEXIALOG</p>"},{"location":"models/evaluation/","title":"\u00c9valuation des mod\u00e8les","text":""},{"location":"models/evaluation/#vue-densemble","title":"Vue d'ensemble","text":"<p>L'\u00e9valuation des mod\u00e8les est cruciale pour s'assurer de la qualit\u00e9 et de la fiabilit\u00e9 des pr\u00e9dictions de valeur r\u00e9siduelle. Cette section d\u00e9taille les m\u00e9triques, m\u00e9thodologies et r\u00e9sultats d'\u00e9valuation.</p>"},{"location":"models/evaluation/#metriques-devaluation","title":"M\u00e9triques d'\u00e9valuation","text":""},{"location":"models/evaluation/#metriques-principales","title":"M\u00e9triques principales","text":"M\u00e9trique Formule Interpr\u00e9tation Usage MAE \\(\\frac{1}{n}\\sum_{i=1}^{n}\\|y_i - \\hat{y_i}\\|\\) Erreur absolue moyenne Erreur typique en \u20ac RMSE \\(\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}\\) P\u00e9nalise les grosses erreurs Sensibilit\u00e9 aux outliers R\u00b2 \\(1 - \\frac{SS_{res}}{SS_{tot}}\\) Variance expliqu\u00e9e (0-1) Qualit\u00e9 globale MAPE \\(\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\|y_i - \\hat{y_i}\\|}{y_i} \\times 100\\) Erreur relative (%) Erreur en pourcentage"},{"location":"models/evaluation/#metriques-metier-specifiques","title":"M\u00e9triques m\u00e9tier sp\u00e9cifiques","text":"<pre><code>def business_metrics(y_true, y_pred, prix_neuf):\n    \"\"\"M\u00e9triques sp\u00e9cifiques au domaine automobile\"\"\"\n\n    # Erreur en euros\n    prix_occasion_true = y_true * prix_neuf\n    prix_occasion_pred = y_pred * prix_neuf\n\n    mae_euros = np.mean(np.abs(prix_occasion_true - prix_occasion_pred))\n\n    # Pourcentage de pr\u00e9dictions dans une marge acceptable (\u00b15%)\n    marge_acceptable = 0.05\n    predictions_acceptables = np.mean(\n        np.abs(y_true - y_pred) &lt;= marge_acceptable\n    )\n\n    # Biais (sous-estimation vs sur-estimation)\n    biais = np.mean(y_pred - y_true)\n\n    return {\n        'mae_euros': mae_euros,\n        'predictions_acceptables_5pc': predictions_acceptables * 100,\n        'biais_taux_decote': biais\n    }\n</code></pre>"},{"location":"models/evaluation/#strategie-de-validation","title":"Strat\u00e9gie de validation","text":""},{"location":"models/evaluation/#1-division-temporelle-des-donnees","title":"1. Division temporelle des donn\u00e9es","text":"<p>Importance de la validation temporelle</p> <p>Les prix des v\u00e9hicules \u00e9voluent dans le temps (inflation, nouveaux mod\u00e8les, crises). Une validation temporelle est essentielle.</p> <pre><code>def temporal_split(df, test_size=0.2):\n    \"\"\"Division temporelle bas\u00e9e sur la date de collecte\"\"\"\n\n    # Tri par date de collecte\n    df_sorted = df.sort_values('date_collecte')\n\n    # Point de coupure\n    split_idx = int(len(df_sorted) * (1 - test_size))\n\n    train_data = df_sorted.iloc[:split_idx]\n    test_data = df_sorted.iloc[split_idx:]\n\n    return train_data, test_data\n</code></pre>"},{"location":"models/evaluation/#2-validation-croisee-temporelle","title":"2. Validation crois\u00e9e temporelle","text":"<pre><code>from sklearn.model_selection import TimeSeriesSplit\n\n# Configuration de la validation crois\u00e9e temporelle\ntscv = TimeSeriesSplit(n_splits=5)\n\ndef temporal_cross_validation(model, X, y):\n    \"\"\"Validation crois\u00e9e avec respect de l'ordre temporel\"\"\"\n\n    scores = {\n        'mae': [],\n        'rmse': [],\n        'r2': [],\n        'mape': []\n    }\n\n    for train_idx, val_idx in tscv.split(X):\n        X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n\n        # Entra\u00eenement\n        model.fit(X_train_fold, y_train_fold)\n\n        # Pr\u00e9diction\n        y_pred = model.predict(X_val_fold)\n\n        # M\u00e9triques\n        scores['mae'].append(mean_absolute_error(y_val_fold, y_pred))\n        scores['rmse'].append(np.sqrt(mean_squared_error(y_val_fold, y_pred)))\n        scores['r2'].append(r2_score(y_val_fold, y_pred))\n        scores['mape'].append(mean_absolute_percentage_error(y_val_fold, y_pred))\n\n    return {metric: (np.mean(values), np.std(values)) \n            for metric, values in scores.items()}\n</code></pre>"},{"location":"models/evaluation/#resultats-des-modeles","title":"R\u00e9sultats des mod\u00e8les","text":""},{"location":"models/evaluation/#comparaison-des-algorithmes","title":"Comparaison des algorithmes","text":"Mod\u00e8le MAE RMSE R\u00b2 MAPE (%) Temps (s) XGBoost 0.087 0.124 0.892 12.3 45 CatBoost 0.091 0.128 0.885 13.1 38 Random Forest 0.095 0.135 0.871 14.2 22 Linear Regression 0.142 0.198 0.712 19.8 2 <p>Mod\u00e8le retenu</p> <p>XGBoost offre les meilleures performances avec un R\u00b2 de 0.892 et une MAE de 0.087.</p>"},{"location":"models/evaluation/#performance-par-segment","title":"Performance par segment","text":"<pre><code>def evaluate_by_segment(model, X_test, y_test, segment_col):\n    \"\"\"\u00c9valuation des performances par segment de v\u00e9hicule\"\"\"\n\n    results = {}\n\n    for segment in X_test[segment_col].unique():\n        mask = X_test[segment_col] == segment\n        X_segment = X_test[mask]\n        y_segment = y_test[mask]\n\n        if len(y_segment) &gt; 10:  # Minimum d'\u00e9chantillons\n            y_pred_segment = model.predict(X_segment)\n\n            results[segment] = {\n                'n_samples': len(y_segment),\n                'mae': mean_absolute_error(y_segment, y_pred_segment),\n                'rmse': np.sqrt(mean_squared_error(y_segment, y_pred_segment)),\n                'r2': r2_score(y_segment, y_pred_segment)\n            }\n\n    return pd.DataFrame(results).T\n</code></pre>"},{"location":"models/evaluation/#resultats-par-segment-de-vehicule","title":"R\u00e9sultats par segment de v\u00e9hicule","text":"Segment \u00c9chantillons MAE RMSE R\u00b2 \u00c9conomique 3,245 0.082 0.118 0.901 Compact 5,678 0.085 0.121 0.895 Familial 4,123 0.089 0.127 0.888 Premium 2,156 0.095 0.138 0.875 <p>Observations</p> <ul> <li>Meilleures performances sur les v\u00e9hicules \u00e9conomiques et compacts</li> <li>L\u00e9g\u00e8re d\u00e9gradation sur le segment premium (plus de variabilit\u00e9)</li> </ul>"},{"location":"models/evaluation/#analyse-des-erreurs","title":"Analyse des erreurs","text":""},{"location":"models/evaluation/#distribution-des-erreurs","title":"Distribution des erreurs","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_error_distribution(y_true, y_pred):\n    \"\"\"Analyse de la distribution des erreurs\"\"\"\n\n    errors = y_pred - y_true\n\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n    # Distribution des erreurs\n    axes[0,0].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n    axes[0,0].axvline(0, color='red', linestyle='--', label='Erreur nulle')\n    axes[0,0].set_title('Distribution des erreurs')\n    axes[0,0].set_xlabel('Erreur (pr\u00e9dit - r\u00e9el)')\n    axes[0,0].legend()\n\n    # Q-Q plot pour normalit\u00e9\n    from scipy import stats\n    stats.probplot(errors, dist=\"norm\", plot=axes[0,1])\n    axes[0,1].set_title('Q-Q Plot (normalit\u00e9 des erreurs)')\n\n    # R\u00e9sidus vs pr\u00e9dictions\n    axes[1,0].scatter(y_pred, errors, alpha=0.5)\n    axes[1,0].axhline(0, color='red', linestyle='--')\n    axes[1,0].set_xlabel('Valeurs pr\u00e9dites')\n    axes[1,0].set_ylabel('R\u00e9sidus')\n    axes[1,0].set_title('R\u00e9sidus vs Pr\u00e9dictions')\n\n    # Valeurs r\u00e9elles vs pr\u00e9dites\n    axes[1,1].scatter(y_true, y_pred, alpha=0.5)\n    axes[1,1].plot([y_true.min(), y_true.max()], \n                   [y_true.min(), y_true.max()], \n                   'r--', linewidth=2)\n    axes[1,1].set_xlabel('Valeurs r\u00e9elles')\n    axes[1,1].set_ylabel('Valeurs pr\u00e9dites')\n    axes[1,1].set_title('R\u00e9el vs Pr\u00e9dit')\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"models/evaluation/#analyse-des-cas-extremes","title":"Analyse des cas extr\u00eames","text":"<pre><code>def analyze_extreme_errors(X_test, y_test, y_pred, threshold=0.15):\n    \"\"\"Analyse des pr\u00e9dictions avec de grosses erreurs\"\"\"\n\n    errors = np.abs(y_pred - y_test)\n    extreme_mask = errors &gt; threshold\n\n    extreme_cases = X_test[extreme_mask].copy()\n    extreme_cases['error'] = errors[extreme_mask]\n    extreme_cases['y_true'] = y_test[extreme_mask]\n    extreme_cases['y_pred'] = y_pred[extreme_mask]\n\n    print(f\"Cas avec erreur &gt; {threshold}: {extreme_mask.sum()} ({extreme_mask.mean()*100:.1f}%)\")\n\n    # Analyse par caract\u00e9ristiques\n    print(\"\\nCaract\u00e9ristiques des cas extr\u00eames:\")\n    for col in ['marque', 'segment', 'carburant']:\n        if col in extreme_cases.columns:\n            print(f\"\\n{col}:\")\n            print(extreme_cases[col].value_counts().head())\n\n    return extreme_cases\n</code></pre>"},{"location":"models/evaluation/#tests-de-robustesse","title":"Tests de robustesse","text":""},{"location":"models/evaluation/#1-stabilite-temporelle","title":"1. Stabilit\u00e9 temporelle","text":"<pre><code>def temporal_stability_test(model, X, y, date_col, window_months=6):\n    \"\"\"Test de stabilit\u00e9 des performances dans le temps\"\"\"\n\n    results = []\n\n    # Fen\u00eatres glissantes\n    dates = pd.to_datetime(X[date_col])\n    start_date = dates.min()\n    end_date = dates.max()\n\n    current_date = start_date\n    while current_date &lt; end_date - pd.DateOffset(months=window_months):\n\n        # Fen\u00eatre de test\n        test_start = current_date\n        test_end = current_date + pd.DateOffset(months=window_months)\n\n        test_mask = (dates &gt;= test_start) &amp; (dates &lt; test_end)\n\n        if test_mask.sum() &gt; 50:  # Minimum d'\u00e9chantillons\n            X_window = X[test_mask]\n            y_window = y[test_mask]\n\n            y_pred_window = model.predict(X_window)\n\n            results.append({\n                'date': test_start,\n                'n_samples': len(y_window),\n                'mae': mean_absolute_error(y_window, y_pred_window),\n                'r2': r2_score(y_window, y_pred_window)\n            })\n\n        current_date += pd.DateOffset(months=1)\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"models/evaluation/#2-test-de-derive-des-donnees","title":"2. Test de d\u00e9rive des donn\u00e9es","text":"<pre><code>from scipy.stats import ks_2samp\n\ndef data_drift_test(X_train, X_test, threshold=0.05):\n    \"\"\"D\u00e9tection de d\u00e9rive dans les distributions des features\"\"\"\n\n    drift_results = {}\n\n    for col in X_train.select_dtypes(include=[np.number]).columns:\n        # Test de Kolmogorov-Smirnov\n        statistic, p_value = ks_2samp(X_train[col], X_test[col])\n\n        drift_results[col] = {\n            'statistic': statistic,\n            'p_value': p_value,\n            'drift_detected': p_value &lt; threshold\n        }\n\n    drift_df = pd.DataFrame(drift_results).T\n\n    print(f\"Features avec d\u00e9rive d\u00e9tect\u00e9e: {drift_df['drift_detected'].sum()}\")\n    print(\"\\nTop 5 d\u00e9rives:\")\n    print(drift_df.sort_values('statistic', ascending=False).head())\n\n    return drift_df\n</code></pre>"},{"location":"models/evaluation/#validation-metier","title":"Validation m\u00e9tier","text":""},{"location":"models/evaluation/#coherence-economique","title":"Coh\u00e9rence \u00e9conomique","text":"<pre><code>def economic_coherence_check(model, test_cases):\n    \"\"\"V\u00e9rification de la coh\u00e9rence \u00e9conomique des pr\u00e9dictions\"\"\"\n\n    coherence_tests = []\n\n    # Test 1: Plus de kilom\u00e9trage = d\u00e9cote plus forte\n    base_case = test_cases.iloc[0].copy()\n\n    for km in [20000, 50000, 80000, 120000]:\n        case = base_case.copy()\n        case['kilometrage'] = km\n        prediction = model.predict([case])[0]\n        coherence_tests.append({\n            'test': 'kilom\u00e9trage_impact',\n            'kilometrage': km,\n            'taux_decote': prediction\n        })\n\n    # Test 2: Plus vieux = d\u00e9cote plus forte\n    for age in [1, 3, 5, 7]:\n        case = base_case.copy()\n        case['age_vehicule'] = age\n        case['annee'] = 2024 - age\n        prediction = model.predict([case])[0]\n        coherence_tests.append({\n            'test': 'age_impact',\n            'age': age,\n            'taux_decote': prediction\n        })\n\n    return pd.DataFrame(coherence_tests)\n</code></pre>"},{"location":"models/evaluation/#rapport-devaluation-final","title":"Rapport d'\u00e9valuation final","text":""},{"location":"models/evaluation/#resume-des-performances","title":"R\u00e9sum\u00e9 des performances","text":"<p>Performances du mod\u00e8le XGBoost</p> <ul> <li>R\u00b2 = 0.892 : Explique 89.2% de la variance</li> <li>MAE = 0.087 : Erreur moyenne de 8.7% sur le taux de d\u00e9cote</li> <li>Erreur moyenne en \u20ac : ~2,300\u20ac sur le prix de vente</li> <li>Pr\u00e9dictions acceptables (\u00b15%) : 73% des cas</li> </ul>"},{"location":"models/evaluation/#points-forts","title":"Points forts","text":"<ul> <li>\u2705 Excellentes performances globales</li> <li>\u2705 Stabilit\u00e9 temporelle valid\u00e9e</li> <li>\u2705 Coh\u00e9rence \u00e9conomique respect\u00e9e</li> <li>\u2705 Robustesse aux valeurs aberrantes</li> </ul>"},{"location":"models/evaluation/#points-damelioration","title":"Points d'am\u00e9lioration","text":"<ul> <li>\ud83d\udd04 Performance moindre sur le segment premium</li> <li>\ud83d\udd04 Sensibilit\u00e9 aux mod\u00e8les rares</li> <li>\ud83d\udd04 Besoin de mise \u00e0 jour r\u00e9guli\u00e8re des donn\u00e9es</li> </ul>"},{"location":"models/modeling/","title":"Mod\u00e9lisation","text":""},{"location":"models/modeling/#vue-densemble","title":"Vue d'ensemble","text":"<p>La phase de mod\u00e9lisation vise \u00e0 pr\u00e9dire la valeur r\u00e9siduelle des v\u00e9hicules d'occasion en utilisant des algorithmes de machine learning avanc\u00e9s.</p>"},{"location":"models/modeling/#objectif","title":"Objectif","text":"<p>Pr\u00e9dire le taux de d\u00e9cote : \\(\\text{Taux de d\u00e9cote} = \\frac{\\text{Prix occasion}}{\\text{Prix neuf}}\\)</p>"},{"location":"models/modeling/#preparation-des-donnees","title":"Pr\u00e9paration des donn\u00e9es","text":""},{"location":"models/modeling/#variables-dentree-features","title":"Variables d'entr\u00e9e (features)","text":"Cat\u00e9gorie Variables Type Exemple V\u00e9hicule marque, modele, annee Cat\u00e9goriel/Num\u00e9rique BMW, X3, 2019 Caract\u00e9ristiques puissance, cylindree, carburant Num\u00e9rique/Cat\u00e9goriel 150 CV, 1998 cm\u00b3, Diesel Usage kilometrage, age_vehicule Num\u00e9rique 45000 km, 3 ans March\u00e9 segment, gamme Cat\u00e9goriel SUV, Premium"},{"location":"models/modeling/#variables-derivees-feature-engineering","title":"Variables d\u00e9riv\u00e9es (feature engineering)","text":"<pre><code># Exemples de features engineered\ndef create_features(df):\n    # Age du v\u00e9hicule\n    df['age_vehicule'] = 2024 - df['annee']\n\n    # Kilom\u00e9trage par an\n    df['km_par_an'] = df['kilometrage'] / df['age_vehicule']\n\n    # Ratio puissance/poids (approxim\u00e9)\n    df['puissance_specifique'] = df['puissance'] / df['cylindree'] * 1000\n\n    # Cat\u00e9gorie de kilom\u00e9trage\n    df['categorie_km'] = pd.cut(df['kilometrage'], \n                               bins=[0, 20000, 50000, 80000, float('inf')],\n                               labels=['Faible', 'Moyen', '\u00c9lev\u00e9', 'Tr\u00e8s \u00e9lev\u00e9'])\n\n    # Segment v\u00e9hicule (bas\u00e9 sur prix neuf)\n    df['segment'] = pd.cut(df['prix_neuf'],\n                          bins=[0, 20000, 35000, 50000, float('inf')],\n                          labels=['\u00c9conomique', 'Compact', 'Familial', 'Premium'])\n\n    return df\n</code></pre>"},{"location":"models/modeling/#algorithmes-testes","title":"Algorithmes test\u00e9s","text":""},{"location":"models/modeling/#1-xgboost-extreme-gradient-boosting","title":"1. XGBoost (Extreme Gradient Boosting)","text":"<p>Avantages : - Excellent pour les donn\u00e9es tabulaires - Gestion native des valeurs manquantes - Robuste au surapprentissage - Interpr\u00e9tabilit\u00e9 via SHAP</p> <pre><code>import xgboost as xgb\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Configuration XGBoost\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 1000,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'random_state': 42\n}\n\n# Entra\u00eenement\nxgb_model = xgb.XGBRegressor(**xgb_params)\nxgb_model.fit(X_train, y_train)\n</code></pre>"},{"location":"models/modeling/#2-catboost","title":"2. CatBoost","text":"<p>Avantages : - Traitement automatique des variables cat\u00e9gorielles - Peu de pr\u00e9processing requis - Performances comp\u00e9titives - Gestion du sur-apprentissage</p> <pre><code>from catboost import CatBoostRegressor\n\n# Variables cat\u00e9gorielles\ncat_features = ['marque', 'modele', 'carburant', 'transmission', 'segment']\n\n# Configuration CatBoost\ncatboost_params = {\n    'iterations': 1000,\n    'depth': 6,\n    'learning_rate': 0.1,\n    'cat_features': cat_features,\n    'verbose': False,\n    'random_seed': 42\n}\n\n# Entra\u00eenement\ncatboost_model = CatBoostRegressor(**catboost_params)\ncatboost_model.fit(X_train, y_train)\n</code></pre>"},{"location":"models/modeling/#3-random-forest","title":"3. Random Forest","text":"<p>Avantages : - Robuste et stable - Peu sensible aux hyperparam\u00e8tres - Importance des features native - Bon baseline</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\n# Configuration Random Forest\nrf_params = {\n    'n_estimators': 200,\n    'max_depth': 15,\n    'min_samples_split': 5,\n    'min_samples_leaf': 2,\n    'random_state': 42,\n    'n_jobs': -1\n}\n\n# Entra\u00eenement\nrf_model = RandomForestRegressor(**rf_params)\nrf_model.fit(X_train, y_train)\n</code></pre>"},{"location":"models/modeling/#pipeline-de-preprocessing","title":"Pipeline de preprocessing","text":"<pre><code>from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n# D\u00e9finition des transformateurs\nnumeric_features = ['kilometrage', 'puissance', 'cylindree', 'age_vehicule']\ncategorical_features = ['marque', 'modele', 'carburant', 'transmission']\n\n# Pipeline pour variables num\u00e9riques\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Pipeline pour variables cat\u00e9gorielles\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n    ('encoder', LabelEncoder())\n])\n\n# Combinaison des transformateurs\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n\n# Pipeline complet\ncomplete_pipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('regressor', xgb.XGBRegressor(**xgb_params))\n])\n</code></pre>"},{"location":"models/modeling/#validation-et-selection-du-modele","title":"Validation et s\u00e9lection du mod\u00e8le","text":""},{"location":"models/modeling/#strategie-de-validation","title":"Strat\u00e9gie de validation","text":"<pre><code>from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\n# Validation temporelle (important pour les donn\u00e9es de prix)\ntscv = TimeSeriesSplit(n_splits=5)\n\n# M\u00e9triques d'\u00e9valuation\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndef evaluate_model(model, X, y, cv=tscv):\n    \"\"\"\u00c9valuation compl\u00e8te d'un mod\u00e8le\"\"\"\n\n    # Cross-validation\n    mae_scores = cross_val_score(model, X, y, cv=cv, \n                                scoring='neg_mean_absolute_error')\n    rmse_scores = cross_val_score(model, X, y, cv=cv, \n                                 scoring='neg_mean_squared_error')\n    r2_scores = cross_val_score(model, X, y, cv=cv, \n                               scoring='r2')\n\n    return {\n        'MAE': -mae_scores.mean(),\n        'RMSE': np.sqrt(-rmse_scores.mean()),\n        'R\u00b2': r2_scores.mean(),\n        'MAE_std': mae_scores.std(),\n        'RMSE_std': np.sqrt(rmse_scores.std()),\n        'R\u00b2_std': r2_scores.std()\n    }\n</code></pre>"},{"location":"models/modeling/#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"models/modeling/#optimisation-bayesienne-avec-optuna","title":"Optimisation bay\u00e9sienne avec Optuna","text":"<pre><code>import optuna\n\ndef objective(trial):\n    \"\"\"Fonction objectif pour l'optimisation des hyperparam\u00e8tres\"\"\"\n\n    # Hyperparam\u00e8tres \u00e0 optimiser\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1)\n    }\n\n    # Entra\u00eenement avec validation crois\u00e9e\n    model = xgb.XGBRegressor(**params, random_state=42)\n    scores = cross_val_score(model, X_train, y_train, cv=tscv, \n                            scoring='neg_mean_absolute_error')\n\n    return -scores.mean()\n\n# Optimisation\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nprint(f\"Meilleurs hyperparam\u00e8tres: {study.best_params}\")\n</code></pre>"},{"location":"models/modeling/#analyse-des-resultats","title":"Analyse des r\u00e9sultats","text":""},{"location":"models/modeling/#importance-des-features","title":"Importance des features","text":"<pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importance XGBoost\nfeature_importance = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Visualisation\nplt.figure(figsize=(10, 8))\nsns.barplot(data=feature_importance.head(15), \n            x='importance', y='feature')\nplt.title('Top 15 - Importance des Features (XGBoost)')\nplt.xlabel('Importance')\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"models/modeling/#analyse-shap","title":"Analyse SHAP","text":"<pre><code>import shap\n\n# Valeurs SHAP pour interpr\u00e9tabilit\u00e9\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_test)\n\n# Graphiques SHAP\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\nshap.summary_plot(shap_values, X_test)\n</code></pre>"},{"location":"models/modeling/#modele-final-selectionne","title":"Mod\u00e8le final s\u00e9lectionn\u00e9","text":"<p>Apr\u00e8s \u00e9valuation, XGBoost a \u00e9t\u00e9 retenu comme mod\u00e8le de production :</p>"},{"location":"models/modeling/#hyperparametres-optimaux","title":"Hyperparam\u00e8tres optimaux","text":"<pre><code>final_params = {\n    'objective': 'reg:squarederror',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'n_estimators': 1500,\n    'subsample': 0.85,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 0.1,\n    'reg_lambda': 0.1,\n    'random_state': 42\n}\n</code></pre>"},{"location":"models/modeling/#sauvegarde-du-modele","title":"Sauvegarde du mod\u00e8le","text":"<pre><code>import joblib\n\n# Sauvegarde du pipeline complet\njoblib.dump(complete_pipeline, 'models/xg_boost_cst_reg.joblib')\n\n# Sauvegarde des transformateurs\njoblib.dump(preprocessor, 'models/transform_pipeline.joblib')\n\nprint(\"\u2705 Mod\u00e8le sauvegard\u00e9 avec succ\u00e8s\")\n</code></pre>"},{"location":"models/modeling/#utilisation-en-production","title":"Utilisation en production","text":"<pre><code># Chargement du mod\u00e8le\nmodel = joblib.load('models/xg_boost_cst_reg.joblib')\n\n# Pr\u00e9diction\ndef predict_residual_value(car_features):\n    \"\"\"Pr\u00e9diction de la valeur r\u00e9siduelle\"\"\"\n    prediction = model.predict([car_features])\n    return prediction[0]\n\n# Exemple d'utilisation\ncar_example = {\n    'marque': 'BMW',\n    'modele': 'X3',\n    'annee': 2020,\n    'kilometrage': 35000,\n    'puissance': 190,\n    'carburant': 'Diesel'\n}\n\ntaux_decote = predict_residual_value(car_example)\nprint(f\"Taux de d\u00e9cote pr\u00e9dit: {taux_decote:.3f}\")\n</code></pre>"},{"location":"models/performance/","title":"Performances des mod\u00e8les","text":""},{"location":"models/performance/#resultats-de-production","title":"R\u00e9sultats de production","text":""},{"location":"models/performance/#metriques-en-temps-reel","title":"M\u00e9triques en temps r\u00e9el","text":"M\u00e9trique Valeur Objectif Statut Pr\u00e9cision moyenne Erreur MAE Temps de pr\u00e9diction Disponibilit\u00e9"},{"location":"projet/overview/","title":"Vue d'ensemble du projet","text":""},{"location":"projet/overview/#contexte-et-objectifs","title":"Contexte et objectifs","text":"<p>Le projet Valeur R\u00e9siduelle vise \u00e0 d\u00e9velopper une solution compl\u00e8te pour l'estimation automatis\u00e9e des prix de v\u00e9hicules d'occasion et de leur courbe de d\u00e9cote.</p>"},{"location":"projet/overview/#problematique","title":"Probl\u00e9matique","text":"<ul> <li>Besoin : Estimation pr\u00e9cise de la valeur r\u00e9siduelle des v\u00e9hicules</li> <li>D\u00e9fis : Multitude de facteurs influen\u00e7ant le prix (marque, mod\u00e8le, ann\u00e9e, kilom\u00e9trage, \u00e9tat, etc.)</li> <li>Solution : Mod\u00e8le de machine learning bas\u00e9 sur des donn\u00e9es r\u00e9elles du march\u00e9</li> </ul>"},{"location":"projet/overview/#architecture-du-projet","title":"Architecture du projet","text":"<pre><code>graph TD\n    A[Collecte de donn\u00e9es] --&gt; B[Preprocessing]\n    B --&gt; C[Feature Engineering]\n    C --&gt; D[Entra\u00eenement du mod\u00e8le]\n    D --&gt; E[\u00c9valuation]\n    E --&gt; F[Application Web]\n\n    A1[autohero.com] --&gt; A\n    A2[caradisiac.com] --&gt; A\n\n    D --&gt; D1[XGBoost]\n    D --&gt; D2[CatBoost]\n    D --&gt; D3[Random Forest]\n\n    F --&gt; F1[Interface utilisateur]\n    F --&gt; F2[API REST]</code></pre>"},{"location":"projet/overview/#methodologie","title":"M\u00e9thodologie","text":""},{"location":"projet/overview/#1-collecte-des-donnees","title":"1. Collecte des donn\u00e9es","text":"<ul> <li>Source 1 : Annonces v\u00e9hicules d'occasion (autohero.com)</li> <li>Source 2 : Fiches techniques et prix neufs (caradisiac.com)</li> <li>Crit\u00e8res de s\u00e9lection :</li> <li>Ann\u00e9e de mise en circulation \u2265 2017</li> <li>Kilom\u00e9trage \u2264 100 000 km</li> </ul>"},{"location":"projet/overview/#2-preprocessing-et-feature-engineering","title":"2. Preprocessing et feature engineering","text":"<ul> <li>Nettoyage des donn\u00e9es manquantes</li> <li>Standardisation des formats</li> <li>Cr\u00e9ation de variables d\u00e9riv\u00e9es</li> <li>Encodage des variables cat\u00e9gorielles</li> </ul>"},{"location":"projet/overview/#3-modelisation","title":"3. Mod\u00e9lisation","text":"<ul> <li>Variable cible : Taux de d\u00e9cote = Prix occasion / Prix neuf</li> <li>Algorithmes test\u00e9s : XGBoost, CatBoost, Random Forest</li> <li>Validation : Cross-validation et jeu de test</li> </ul>"},{"location":"projet/overview/#4-deploiement","title":"4. D\u00e9ploiement","text":"<ul> <li>Application web interactive avec Dash</li> <li>API REST pour int\u00e9gration</li> <li>Interface utilisateur intuitive</li> </ul>"},{"location":"projet/overview/#livrables","title":"Livrables","text":""},{"location":"projet/overview/#phase-1-modele-ml","title":"Phase 1 - Mod\u00e8le ML","text":"<ul> <li> Dataset nettoy\u00e9 et pr\u00e9process\u00e9</li> <li> Mod\u00e8les entra\u00een\u00e9s et \u00e9valu\u00e9s</li> <li> Pipeline de preprocessing</li> <li> M\u00e9triques de performance</li> </ul>"},{"location":"projet/overview/#phase-2-application","title":"Phase 2 - Application","text":"<ul> <li> Interface web fonctionnelle</li> <li> API REST document\u00e9e</li> <li> Tests automatis\u00e9s</li> <li> Documentation compl\u00e8te</li> </ul>"},{"location":"projet/overview/#equipe-et-responsabilites","title":"\u00c9quipe et responsabilit\u00e9s","text":"R\u00f4le Responsabilit\u00e9 Data Scientist Mod\u00e9lisation et feature engineering Data Engineer Pipeline de donn\u00e9es et preprocessing D\u00e9veloppeur Web Application et interface utilisateur DevOps D\u00e9ploiement et infrastructure"},{"location":"projet/overview/#planning","title":"Planning","text":"<pre><code>gantt\n    title Planning Projet Valeur R\u00e9siduelle\n    dateFormat  YYYY-MM-DD\n    section Phase 1\n    Collecte donn\u00e9es    :2024-01-01, 2024-02-01\n    Preprocessing       :2024-02-01, 2024-02-15\n    Mod\u00e9lisation        :2024-02-15, 2024-03-15\n    \u00c9valuation          :2024-03-15, 2024-03-31\n    section Phase 2\n    D\u00e9veloppement app   :2024-04-01, 2024-05-15\n    Tests et validation :2024-05-15, 2024-05-31\n    D\u00e9ploiement         :2024-06-01, 2024-06-15</code></pre>"}]}